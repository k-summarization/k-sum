{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud55c\uad6d\uc5b4 \ubb38\uc11c\uc694\uc57d \ubb38\uc11c\uc694\uc57d\uc758 \uc138\uacc4\uc5d0 \uc624\uc2e0\uac78 \ud658\uc601\ud569\ub2c8\ub2e4. Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"\ud55c\uad6d\uc5b4 \ubb38\uc11c\uc694\uc57d"},{"location":"#_1","text":"\ubb38\uc11c\uc694\uc57d\uc758 \uc138\uacc4\uc5d0 \uc624\uc2e0\uac78 \ud658\uc601\ud569\ub2c8\ub2e4.","title":"\ud55c\uad6d\uc5b4 \ubb38\uc11c\uc694\uc57d"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"BERTSUM/","text":"BertSum BertSum Extractive BertSum Abstractive","title":"BertSum"},{"location":"BERTSUM/#bertsum","text":"","title":"BertSum"},{"location":"BERTSUM/#bertsum-extractive","text":"","title":"BertSum Extractive"},{"location":"BERTSUM/#bertsum-abstractive","text":"","title":"BertSum Abstractive"},{"location":"Neusum/","text":"Neusum","title":"Neusum"},{"location":"Neusum/#neusum","text":"","title":"Neusum"},{"location":"PointerGenerator/","text":"PointerGenerator","title":"PointerGenerator"},{"location":"PointerGenerator/#pointergenerator","text":"","title":"PointerGenerator"},{"location":"TextRank/","text":"TextRank","title":"TextRank"},{"location":"TextRank/#textrank","text":"","title":"TextRank"},{"location":"docs-pointergenerator/","text":"Get To The Point: Summarization with Pointer-Generator Networks \uc0ac\uc6a9\ud558\ub294 \ub370\uc774\ud130\ub294 \uc544\ub798\uc640 \uac19\uc740 \ud615\uc2dd\uc744 \uac16\uc2b5\ub2c8\ub2e4. { \"media\": \"AA\uc77c\ubcf4\", \"id\": \"123456789\", \"article_original\": [ \"\uc624\ub298 \uc544\uce68 \ubbf8\ud655\uc778 \ube44\ud589\ubb3c\uccb4\uac00 \ub4f1\uc7a5\ud588\uc2b5\ub2c8\ub2e4.\", \"\uc11c\uc6b8 \uadfc\uad50\uc758 \ud55c \uc232\uc5d0\uc11c \uad00\uce21\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\", \"\ubbf8\ud655\uc778 \ube44\ud589\ubb3c\uccb4\ub294 \ucd5c\uadfc \ub4e4\uc5b4 \ub9ce\uc774 \uad00\uce21\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4.\", \"\uc2dc\ubbfc\ub4e4\uc740 \ud638\uae30\uc2ec\ubcf4\ub2e4 \ubd88\uc548\ud55c \ub9c8\uc74c\uc744 \uac16\uace0 \uc788\uc2b5\ub2c8\ub2e4.\", \"\uc11c\uc6b8\uc2dc\ub294 \ud574\ub2f9 \ubb3c\uccb4\uc5d0 \ub300\ud55c \uc870\uc0ac\ub97c \ud55c\ub2e4\ub294 \uc785\uc7a5\ubb38\uc744 \ubc1d\ud614\uc2b5\ub2c8\ub2e4.\", \"\ubbf8\ud655\uc778 \ube44\ud589\ubb3c\uccb4\uac00 \uc55e\uc73c\ub85c \ub354 \ub4f1\uc7a5\ud560 \uc9c0 \uc758\ubb38\uc785\ub2c8\ub2e4.\", ], \"abstractive\": \"\uac11\uc791\uc2a4\ub808 \ub4f1\uc7a5\ud55c \ubbf8\ud655\uc778 \ube44\ud589\ubb3c\uccb4\uac00 \uc11c\uc6b8\uc2dc\ubbfc\uc744 \ubd88\uc548\ud558\uac8c \ub9cc\ub4e4\uace0 \uc788\ub2e4.\", \"extractive\": [0, 3, 4] } TextDataset __init__ path(str), max_vocab(int) = 50000, mode(str) = \"train\", vocab_dir(str) = None, use_sentencepiece(bool) = True Parameters path: train/eval/test \ub370\uc774\ud130\uc14b\uc758 \uacbd\ub85c max_vocab: \uc5b4\ud718 \uc0ac\uc804\uc758 \ucd5c\ub300 \uac1c\uc218 mode: train/eval/test \ubaa8\ub4dc vocab_dir: \uae30\uc874\uc5d0 \uc0dd\uc131\ub41c \uc5b4\ud718 \uc0ac\uc804\uc758 \uacbd\ub85c use_sentencepiece: Sentencepiece \ub610\ub294 Mecab \uc0ac\uc6a9 \uc5ec\ubd80 \uacb0\uc815 get_vocab src(List[List[str]]) \uc8fc\uc5b4\uc9c4 \ubb38\uc11c\ub85c\ubd80\ud130 \uc5b4\ud718 \uc0ac\uc804\uc744 \uc0dd\uc131\ud568 Parameters src: \uc8fc\uc5b4\uc9c4 \uc804\uccb4 \ubb38\uc11c \ub9ac\uc2a4\ud2b8 Returns vocab(dict): \ubb38\uc11c\ub85c\ubd80\ud130 \uc0dd\uc131\ub41c \uc5b4\ud718 \uc0ac\uc804 src2idx src_sentence(List[str]) \uc8fc\uc5b4\uc9c4 \ubb38\uc7a5\uc73c\ub85c\ubd80\ud130 \ub2e8\uc5b4 \uc778\ub371\uc2a4 \ub9ac\uc2a4\ud2b8, OOV\uac00 \ub4f1\uc7a5\ud55c \uc704\uce58\ub97c \uc800\uc7a5\ud558\ub294 \uc0ac\uc804\uc744 \uc0dd\uc131\ud568 Parameters src_sentence: \uc6d0\ubcf8 \ubb38\uc7a5 Returns idcs(list): \uc778\ub371\uc2a4\ub85c \ubcc0\ud658\ub41c \uc6d0\ubcf8 \ubb38\uc7a5, oov_vocabs(dict): OOV\uac00 \ub4f1\uc7a5\ud55c \uc704\uce58 \uc778\ub371\uc2a4 Example > {'oov_word_1': [idx_1], 'oov_word_2':[idx_2], ...} tgt2idx tgt_sentence(List[str]), src_oov_vocabs(List[str]) \uc8fc\uc5b4\uc9c4 \uc694\uc57d\ubb38\uc73c\ub85c\ubd80\ud130 \ub2e8\uc5b4 \uc778\ub371\uc2a4 \ub9ac\uc2a4\ud2b8, OOV\uac00 \ub4f1\uc7a5\ud55c \uc704\uce58\ub97c \uc800\uc7a5\ud558\ub294 \uc0ac\uc804\uc744 \uc0dd\uc131\ud568 (\ub2e8, \uc6d0\ubcf8 \ubb38\uc7a5\uc5d0 \ud574\ub2f9 OOV\uac00 \ub4f1\uc7a5\ud588\uc744 \ub54c\uc5d0\ub9cc \uc0ac\uc804\uc5d0 \uc800\uc7a5) Parameters tgt_sentence: \uc694\uc57d\ubb38 src_oov_vocabs: \uc6d0\ubcf8 \ubb38\uc7a5\uc5d0 \ub4f1\uc7a5\ud55c OOV \uc0ac\uc804 Returns idcs(list): \uc778\ub371\uc2a4\ub85c \ubcc0\ud658\ub41c \uc694\uc57d\ubb38(Decoding \uc2dc\uc791\uacfc \ub05d\uc5d0 \ud574\ub2f9\ud558\ub294 special token \ud3ec\ud568) oov_vocabs(dict): \uc6d0\ubcf8 \ubb38\uc7a5\uacfc \uc694\uc57d\ubb38\uc5d0 \ud568\uaed8 \ub4f1\uc7a5\ud558\ub294 OOV \ub2e8\uc5b4 \uc0ac\uc804 idx2word sentence(List[str]) \uc778\ub371\uc2a4\ub97c \ub2e8\uc5b4(\ud1a0\ud070)\ub85c \ubc14\uafb8\uc5b4 \uc90c Parameters sentence(List[int] or torch.Tensor): \uc778\ub371\uc2a4\ub85c \uad6c\uc131\ub41c \ubb38\uc7a5 Returns converted_sentence(List[str]): \ub2e8\uc5b4(\ud1a0\ud070)\ub85c \ubcc0\ud658\ub41c \ubb38\uc7a5 __len__ \ub370\uc774\ud130\uc14b\uc758 \uae38\uc774\ub97c \ubc18\ud658 Returns length(int): \ub370\uc774\ud130\uc14b\uc758 \uae38\uc774 __getitem__ idx(int) \uc778\ub371\uc2a4\uc5d0 \ud574\ub2f9\ud558\ub294 \ub370\uc774\ud130\ub97c \ubc18\ud658 Parameters idx(int): \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc815 \uc778\ub371\uc2a4 Returns src[idx]: Tokenize \ud6c4 \uc778\ub371\uc2a4\ub85c \ubcc0\ud658\ub41c \uc6d0\ubcf8 \ubb38\uc7a5 tgt[idx]: Tokenize \ud6c4 \uc778\ub371\uc2a4\ub85c \ubcc0\ud658\ub41c \uc694\uc57d\ubb38 src_oov_vocabs[idx]: \uc6d0\ubcf8 \ubb38\uc7a5\uc5d0 \ub4f1\uc7a5\ud55c OOV \uc0ac\uc804 tgt_oov_vocabs[idx]: \uc6d0\ubcf8 \ubb38\uc7a5\uacfc \uc694\uc57d\ubb38\uc5d0 \ud568\uaed8 \ub4f1\uc7a5\ud558\ub294 OOV \ub2e8\uc5b4 \uc0ac\uc804 PointerGenerator __init__ train_path(str), validation_path(str), use_sentencepiece(bool), vocab_size(int), embed_dim(int), hidden_dim(int), num_encoder_layer(int), max_len(int), max_decoder_step(int), lr(float), train_batch_size(int), eval_batch_size(int), ptr_gen(bool), coverage(bool), cov_loss_lambda(float), vocab_path(str) = None, test_path(str) = None Parameters train_path: \ud559\uc2b5 \ub370\uc774\ud130 \uacbd\ub85c validation_path: \uac80\uc99d \ub370\uc774\ud130 \uacbd\ub85c use_sentencepiece: Sentencepiece \ub610\ub294 Mecab \uc0ac\uc6a9 \uc5ec\ubd80 \uacb0\uc815 vocab_size: \uc5b4\ud718 \uc0ac\uc804 \ud06c\uae30 embed_dim: \ub2e8\uc5b4 \uc784\ubca0\ub529 \ud06c\uae30 hidden_dim: \uc2e0\uacbd\ub9dd \uc740\ub2c9\uce35 \ud06c\uae30 num_encoder_layer: \uc778\ucf54\ub354\uc758 \uce35 \uac1c\uc218 max_len: \ubb38\uc7a5\uc758 \ucd5c\ub300 \uae38\uc774 max_decoder_step: \ub514\ucf54\ub354\uc5d0 \uc758\ud574 \uc0dd\uc131\ub418\ub294 \ubb38\uc7a5\uc758 \ucd5c\ub300 \uae38\uc774 lr: \ud559\uc2b5\uc728(learning rate) train_batch_size: \ud559\uc2b5 \ubc30\uce58 \ud06c\uae30 eval_batch_size: \uac80\uc99d \ubc30\uce58 \ud06c\uae30 ptr_gen: pointer-generator \ubaa8\ub378 \uc0ac\uc6a9 coverage: coverage mechanism \uc801\uc6a9 cov_loss_lambda: coverage loss\uc758 \uac15\ub3c4 \uc870\uc808 \uacc4\uc218 vocab_path: \uae30\uc874\uc5d0 \uc0dd\uc131\ub41c \uc5b4\ud718 \uc0ac\uc804\uc758 \uacbd\ub85c test_path: \ud3c9\uac00(\ud14c\uc2a4\ud2b8) \ub370\uc774\ud130 \uacbd\ub85c forward src(torch.tensor), tgt_input(torch.tensor), src_len(torch.tensor), src_padding_mask(torch.tensor), vocab(dict), src_oov(torch.tensor), content_selection(torch.tensor) = None Parameters src: tgt_input: src_len: src_padding_mask: vocab: src_oov: content_selection: Returns training_step validation_step validation_epoch_end","title":"Docs pointergenerator"},{"location":"docs-pointergenerator/#textdataset","text":"","title":"TextDataset"},{"location":"docs-pointergenerator/#__init__","text":"path(str), max_vocab(int) = 50000, mode(str) = \"train\", vocab_dir(str) = None, use_sentencepiece(bool) = True","title":"__init__"},{"location":"docs-pointergenerator/#parameters","text":"path: train/eval/test \ub370\uc774\ud130\uc14b\uc758 \uacbd\ub85c max_vocab: \uc5b4\ud718 \uc0ac\uc804\uc758 \ucd5c\ub300 \uac1c\uc218 mode: train/eval/test \ubaa8\ub4dc vocab_dir: \uae30\uc874\uc5d0 \uc0dd\uc131\ub41c \uc5b4\ud718 \uc0ac\uc804\uc758 \uacbd\ub85c use_sentencepiece: Sentencepiece \ub610\ub294 Mecab \uc0ac\uc6a9 \uc5ec\ubd80 \uacb0\uc815","title":"Parameters"},{"location":"docs-pointergenerator/#get_vocab","text":"src(List[List[str]]) \uc8fc\uc5b4\uc9c4 \ubb38\uc11c\ub85c\ubd80\ud130 \uc5b4\ud718 \uc0ac\uc804\uc744 \uc0dd\uc131\ud568","title":"get_vocab"},{"location":"docs-pointergenerator/#parameters_1","text":"src: \uc8fc\uc5b4\uc9c4 \uc804\uccb4 \ubb38\uc11c \ub9ac\uc2a4\ud2b8","title":"Parameters"},{"location":"docs-pointergenerator/#returns","text":"vocab(dict): \ubb38\uc11c\ub85c\ubd80\ud130 \uc0dd\uc131\ub41c \uc5b4\ud718 \uc0ac\uc804","title":"Returns"},{"location":"docs-pointergenerator/#src2idx","text":"src_sentence(List[str]) \uc8fc\uc5b4\uc9c4 \ubb38\uc7a5\uc73c\ub85c\ubd80\ud130 \ub2e8\uc5b4 \uc778\ub371\uc2a4 \ub9ac\uc2a4\ud2b8, OOV\uac00 \ub4f1\uc7a5\ud55c \uc704\uce58\ub97c \uc800\uc7a5\ud558\ub294 \uc0ac\uc804\uc744 \uc0dd\uc131\ud568","title":"src2idx"},{"location":"docs-pointergenerator/#parameters_2","text":"src_sentence: \uc6d0\ubcf8 \ubb38\uc7a5","title":"Parameters"},{"location":"docs-pointergenerator/#returns_1","text":"idcs(list): \uc778\ub371\uc2a4\ub85c \ubcc0\ud658\ub41c \uc6d0\ubcf8 \ubb38\uc7a5, oov_vocabs(dict): OOV\uac00 \ub4f1\uc7a5\ud55c \uc704\uce58 \uc778\ub371\uc2a4 Example > {'oov_word_1': [idx_1], 'oov_word_2':[idx_2], ...}","title":"Returns"},{"location":"docs-pointergenerator/#tgt2idx","text":"tgt_sentence(List[str]), src_oov_vocabs(List[str]) \uc8fc\uc5b4\uc9c4 \uc694\uc57d\ubb38\uc73c\ub85c\ubd80\ud130 \ub2e8\uc5b4 \uc778\ub371\uc2a4 \ub9ac\uc2a4\ud2b8, OOV\uac00 \ub4f1\uc7a5\ud55c \uc704\uce58\ub97c \uc800\uc7a5\ud558\ub294 \uc0ac\uc804\uc744 \uc0dd\uc131\ud568 (\ub2e8, \uc6d0\ubcf8 \ubb38\uc7a5\uc5d0 \ud574\ub2f9 OOV\uac00 \ub4f1\uc7a5\ud588\uc744 \ub54c\uc5d0\ub9cc \uc0ac\uc804\uc5d0 \uc800\uc7a5)","title":"tgt2idx"},{"location":"docs-pointergenerator/#parameters_3","text":"tgt_sentence: \uc694\uc57d\ubb38 src_oov_vocabs: \uc6d0\ubcf8 \ubb38\uc7a5\uc5d0 \ub4f1\uc7a5\ud55c OOV \uc0ac\uc804","title":"Parameters"},{"location":"docs-pointergenerator/#returns_2","text":"idcs(list): \uc778\ub371\uc2a4\ub85c \ubcc0\ud658\ub41c \uc694\uc57d\ubb38(Decoding \uc2dc\uc791\uacfc \ub05d\uc5d0 \ud574\ub2f9\ud558\ub294 special token \ud3ec\ud568) oov_vocabs(dict): \uc6d0\ubcf8 \ubb38\uc7a5\uacfc \uc694\uc57d\ubb38\uc5d0 \ud568\uaed8 \ub4f1\uc7a5\ud558\ub294 OOV \ub2e8\uc5b4 \uc0ac\uc804","title":"Returns"},{"location":"docs-pointergenerator/#idx2word","text":"sentence(List[str]) \uc778\ub371\uc2a4\ub97c \ub2e8\uc5b4(\ud1a0\ud070)\ub85c \ubc14\uafb8\uc5b4 \uc90c","title":"idx2word"},{"location":"docs-pointergenerator/#parameters_4","text":"sentence(List[int] or torch.Tensor): \uc778\ub371\uc2a4\ub85c \uad6c\uc131\ub41c \ubb38\uc7a5","title":"Parameters"},{"location":"docs-pointergenerator/#returns_3","text":"converted_sentence(List[str]): \ub2e8\uc5b4(\ud1a0\ud070)\ub85c \ubcc0\ud658\ub41c \ubb38\uc7a5","title":"Returns"},{"location":"docs-pointergenerator/#__len__","text":"\ub370\uc774\ud130\uc14b\uc758 \uae38\uc774\ub97c \ubc18\ud658","title":"__len__"},{"location":"docs-pointergenerator/#returns_4","text":"length(int): \ub370\uc774\ud130\uc14b\uc758 \uae38\uc774","title":"Returns"},{"location":"docs-pointergenerator/#__getitem__","text":"idx(int) \uc778\ub371\uc2a4\uc5d0 \ud574\ub2f9\ud558\ub294 \ub370\uc774\ud130\ub97c \ubc18\ud658","title":"__getitem__"},{"location":"docs-pointergenerator/#parameters_5","text":"idx(int): \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc815 \uc778\ub371\uc2a4","title":"Parameters"},{"location":"docs-pointergenerator/#returns_5","text":"src[idx]: Tokenize \ud6c4 \uc778\ub371\uc2a4\ub85c \ubcc0\ud658\ub41c \uc6d0\ubcf8 \ubb38\uc7a5 tgt[idx]: Tokenize \ud6c4 \uc778\ub371\uc2a4\ub85c \ubcc0\ud658\ub41c \uc694\uc57d\ubb38 src_oov_vocabs[idx]: \uc6d0\ubcf8 \ubb38\uc7a5\uc5d0 \ub4f1\uc7a5\ud55c OOV \uc0ac\uc804 tgt_oov_vocabs[idx]: \uc6d0\ubcf8 \ubb38\uc7a5\uacfc \uc694\uc57d\ubb38\uc5d0 \ud568\uaed8 \ub4f1\uc7a5\ud558\ub294 OOV \ub2e8\uc5b4 \uc0ac\uc804","title":"Returns"},{"location":"docs-pointergenerator/#pointergenerator","text":"","title":"PointerGenerator"},{"location":"docs-pointergenerator/#__init___1","text":"train_path(str), validation_path(str), use_sentencepiece(bool), vocab_size(int), embed_dim(int), hidden_dim(int), num_encoder_layer(int), max_len(int), max_decoder_step(int), lr(float), train_batch_size(int), eval_batch_size(int), ptr_gen(bool), coverage(bool), cov_loss_lambda(float), vocab_path(str) = None, test_path(str) = None","title":"__init__"},{"location":"docs-pointergenerator/#parameters_6","text":"train_path: \ud559\uc2b5 \ub370\uc774\ud130 \uacbd\ub85c validation_path: \uac80\uc99d \ub370\uc774\ud130 \uacbd\ub85c use_sentencepiece: Sentencepiece \ub610\ub294 Mecab \uc0ac\uc6a9 \uc5ec\ubd80 \uacb0\uc815 vocab_size: \uc5b4\ud718 \uc0ac\uc804 \ud06c\uae30 embed_dim: \ub2e8\uc5b4 \uc784\ubca0\ub529 \ud06c\uae30 hidden_dim: \uc2e0\uacbd\ub9dd \uc740\ub2c9\uce35 \ud06c\uae30 num_encoder_layer: \uc778\ucf54\ub354\uc758 \uce35 \uac1c\uc218 max_len: \ubb38\uc7a5\uc758 \ucd5c\ub300 \uae38\uc774 max_decoder_step: \ub514\ucf54\ub354\uc5d0 \uc758\ud574 \uc0dd\uc131\ub418\ub294 \ubb38\uc7a5\uc758 \ucd5c\ub300 \uae38\uc774 lr: \ud559\uc2b5\uc728(learning rate) train_batch_size: \ud559\uc2b5 \ubc30\uce58 \ud06c\uae30 eval_batch_size: \uac80\uc99d \ubc30\uce58 \ud06c\uae30 ptr_gen: pointer-generator \ubaa8\ub378 \uc0ac\uc6a9 coverage: coverage mechanism \uc801\uc6a9 cov_loss_lambda: coverage loss\uc758 \uac15\ub3c4 \uc870\uc808 \uacc4\uc218 vocab_path: \uae30\uc874\uc5d0 \uc0dd\uc131\ub41c \uc5b4\ud718 \uc0ac\uc804\uc758 \uacbd\ub85c test_path: \ud3c9\uac00(\ud14c\uc2a4\ud2b8) \ub370\uc774\ud130 \uacbd\ub85c","title":"Parameters"},{"location":"docs-pointergenerator/#forward","text":"src(torch.tensor), tgt_input(torch.tensor), src_len(torch.tensor), src_padding_mask(torch.tensor), vocab(dict), src_oov(torch.tensor), content_selection(torch.tensor) = None","title":"forward"},{"location":"docs-pointergenerator/#parameters_7","text":"src: tgt_input: src_len: src_padding_mask: vocab: src_oov: content_selection:","title":"Parameters"},{"location":"docs-pointergenerator/#returns_6","text":"","title":"Returns"},{"location":"docs-pointergenerator/#training_step","text":"","title":"training_step"},{"location":"docs-pointergenerator/#validation_step","text":"","title":"validation_step"},{"location":"docs-pointergenerator/#validation_epoch_end","text":"","title":"validation_epoch_end"},{"location":"presumm/","text":"Presumm 1. Overview Presumm\uc740 Bertsum\uc758 \uc800\uc790\uac00 \ubc1c\ud45c\ud55c \ud6c4\uc18d \ub17c\ubb38\uc73c\ub85c, Extractive summarization\uc744 \ubaa9\uc801\uc73c\ub85c \ud55c Bertsum\uc744 \ud65c\uc6a9\ud558\uc5ec Extractive summarization model \uacfc Abstractive summarization \uc774 \ubaa8\ub450 \uac00\ub2a5\ud55c \ub450\uac00\uc9c0 \ubaa8\ub378\uc744 \uc81c\uc548\ud55c\ub2e4. 1) Presumm for Extractive summarization \uae30\uc874\uc758 BERT input\uacfc \ub2e4\ub974\uac8c \ubaa8\ub4e0 \ubb38\uc7a5 \uc55e\ub4a4\uc5d0 [CLS], [SEP] token\uc744 \ubc30\uce58\ud568 \ud55c\uad6d\uc5b4 \ubaa8\ub378\uc744 \uad6c\ud604\ud558\uae30\uc704\ud574 SKT\uc5d0\uc11c \uacf5\uac1c\ud55c KoBERT \ubaa8\ub378\uc744 \ud65c\uc6a9\ud568 KoBERT\uc758 \uc544\uc6c3\ud48b \ubca1\ud130\ub97c \ubaa8\ub450 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 CLS token\uc758 Top vector\ub9cc \uc0ac\uc6a9\ud558\uc5ec Transformer Encoder Layer\ub97c \ud1b5\uacfc\ud568 (\uc0ac\uc6a9\ud55c \ub808\uc774\uc5b4 \uac2f\uc218\ub294 2\uac1c) 2) Presumm for Abstractive summarization Encoder\ub294 KoBERT \ud639\uc740 Pretrained Extractive Summarization Model\uc744 \uc0ac\uc6a9\ud558\uace0, Decoder\ub294 Transformer Decoder\ub97c \uc0ac\uc6a9\ud558\ub294 \ubaa8\ub378 \uad6c\uc870 Encoder\uc758 Input\uc740 Extractive model\uacfc \ub3d9\uc77c\ud558\uac8c \ud558\ub098\uc758 \ubb38\uc7a5 \uc55e\ub4a4\uc5d0 [CLS], [SEP] token\uc744 \ubd99\uc784 Decoder\ub294 \ubcc4\ub3c4\uc758 Special token\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc9c0\ub9cc, \ubb38\uc7a5 \uc55e\ub4a4\uc5d0 [BOS], [EOS] \ub97c \ub354\ud558\uc5ec Document\uc758 \uc2dc\uc791\uacfc \ub05d\uc744 \ud45c\uc2dc\ud568 Extractive model\uacfc \ub2e4\ub974\uac8c Document \uc804\uccb4 \ubca1\ud130\ub97c \ubaa8\ub450 \ud65c\uc6a9\ud558\uc5ec Decoder\uc5d0 \ub123\uc5b4\uc90c Beam search\ub97c \ud65c\uc6a9\ud558\uc5ec \ucd5c\uc885 Output vocab\uc744 \ucc3e\uc544\ub0c4 2. Code scheme Presumm \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 utils \u2502 \u251c\u2500\u2500 dataset.py \u2502 \u251c\u2500\u2500 greedy.py \u2502 \u251c\u2500\u2500 modeling_utils.py \u2502 \u2514\u2500\u2500 preprocess.py \u251c\u2500\u2500 ext \u2502 \u251c\u2500\u2500 modeling_bertext.py \u2502 \u251c\u2500\u2500 ext_train.py \u2502 \u251c\u2500\u2500 ext_trainer.py \u2502 \u251c\u2500\u2500 inference.py \u2502 \u2514\u2500\u2500 ext_test.py \u251c\u2500\u2500 abs \u2502 \u251c\u2500\u2500 modeling_bertabs.py \u2502 \u251c\u2500\u2500 abs_train.py \u2502 \u251c\u2500\u2500 abs_trainer.py \u2502 \u251c\u2500\u2500 abs_test.py \u2502 \u251c\u2500\u2500 abs_generate.py \u2502 \u2514\u2500\u2500 train.sh 3. Utils 1) dataset.py Class Bertsum_Dataset data_path : str, max_len = 512 , train = True, mode = 'ext', language = 'kor' Bertsum\uc5d0 \uc0ac\uc6a9\ub420 \ub370\uc774\ud130\uc14b class\uc774\uba70, Extractive summarization\uacfc Abstract summarization \ubaa8\ub450 \uc801\uc6a9 \uac00\ub2a5\ud55c \ud568\uc218\ub85c mode \ub97c \uc815\ud655\ud788 \ubc14\uafb8\uc5b4\uc8fc\uc5b4\uc57c\ud55c\ub2e4. \ub610\ud55c \ud559\uc2b5/\ucd94\ub860 \uc5ec\ubd80\uc5d0 \ub530\ub77c Train option\uc744 \ubc14\uafb8\uc5b4\uc11c \uc0ac\uc6a9\ud574\uc57c\ud558\uba70 Dataloader\ub97c \uc0ac\uc6a9\ud560\ub54c Dataset \uc635\uc158\uc5d0 \ud574\ub2f9 class\ub97c import\ud558\uace0 \uc0ac\uc6a9\ud574\uc57c\ud55c\ub2e4. Parameters data_path ( str , require) max_len ( int ,optional, defaults to 512 ) train ( bool , optional, defaults to True ) mode ( str ) if string, ext , abs are supported language ( str , defaults to 'kor') def Collate batch Collate \ud568\uc218\ub294 \ubcf4\ud1b5 Data Loader\uc758 \ubc30\uce58 \ub0b4 \ub370\uc774\ud130\ub4e4\uc774 \uae38\uc774\ub098 \uad6c\uc131\uc744 \uac19\uac8c \ud574\uc8fc\ub294 \ud568\uc218\uc774\ub2e4. \ud574\ub2f9 Collate Function\uc774 \uc9c4\ud589\ud558\ub294 \ucc98\ub9ac\ub294 \ub370\uc774\ud130\uac04 \uae38\uc774\ub97c \ub9de\ucdb0\uc8fc\uae30 \uc704\ud574 \ud328\ub529\uc744 \uc2e4\uc2dc\ud574\uc57c \ud558\ub294\ub370 \ubb38\uc7a5\uc774 Max Length\ub97c \ub118\uc5b4\uac08 \uacbd\uc6b0, \ud574\ub2f9 Max Length \uc774\ud558\uc758 \ub9c8\uc9c0\ub9c9 [CLS] Token\uc758 \uc704\uce58\ub97c \ucc3e\uc544 \ubb38\uc7a5 \ud328\ub529\uc744 \uc9c4\ud589\ud55c\ub2e4. def src_add_pad token_ids, token_type token_ids \ub294 \uc804\uccb4 document\ub97c token\ud654 \ud55c \ud6c4 vocab\uc5d0 \ub530\ub77c id\ub85c \ubcc0\uacbd\ud55c \uac83\uc744 \uc758\ubbf8\ud558\uba70, token_type \uc740 document\uc758 \ubb38\uc7a5\uc744 \uad6c\ubcc4\ud574\uc8fc\ub294 \uc2dd\ubcc4\uc790\uc774\ub2e4. \ud574\ub2f9 \ud568\uc218\ub294 source document\ub97c max_length\uc5d0 \ub530\ub77c padding\uc744 \uc9c4\ud589\ud574\uc8fc\ub294 \ud568\uc218 def tgt_add_pad token_ids \ud574\ub2f9 \ud568\uc218\ub294 target document\ub97c max_length\uc5d0 \ub530\ub77c padding\uc744 \uc9c4\ud589\ud574\uc8fc\ub294 \ud568\uc218 def add_special_token token_ids \ud574\ub2f9 \ud568\uc218\ub294 source document\uc5d0\uc11c \ud55c \ubb38\uc7a5\ub9c8\ub2e4 special token\uc778 [CLS]\uc640 [SEP]\ub85c \uac10\uc2f8\uc8fc\ub294 \ud568\uc218 def add_sentence_token token_ids \ud574\ub2f9 \ud568\uc218\ub294 target document \uc804\uccb4\ub97c [BOS], [EOS]\ub85c \uac10\uc2f8\uc8fc\ub294 \uc5ed\ud560\uc744 \ud558\uba70 \ubb38\uc7a5\ub2e8\uc704\uac00 \uc544\ub2c8\ub77c \uc804\uccb4 document\ub97c \uac10\uc2f8\uc8fc\ub294 \ud568\uc218 def idx2mask token_ids padding index\ub85c \uc774\ub8e8\uc5b4\uc9c4 sequence\ub97c mask\ub85c \ubc14\uafb8\uc5b4\uc8fc\ub294 \ud568\uc218 def get_token_type_ids src_token_padded source document\uc758 seuquence\uac00 \uc874\uc7ac\ud560 \ub54c \ubb38\uc7a5 \ub2e8\uc704\ub85c \uad6c\ubcc4\ud558\uae30 \uc704\ud574 type id\ub97c \ubd80\uc5ec\ud558\ub294 \ud568\uc218 def get_cls_index src_token_padded Extractive summarization\uc5d0\uc11c\ub9cc \uc0ac\uc6a9\ub418\ub294 \ud568\uc218\ub85c cls token\uc758 \uc704\uce58\ub97c \uc54c\uc544\ub0b4\uae30\uc704\ud574 cls index \uc704\uce58\ub97c \ucc3e\ub294 \ud568\uc218 def _ getitem _ idx Pytorch Dataset\uc744 \ub9cc\ub4e4\uae30\uc704\ud55c \ud568\uc218\ub85c, \ud559\uc2b5\uacfc \ucd94\ub860\uc2dc batch \ub2e8\uc704\ub85c \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc62c \uc218 \uc788\ub3c4\ub85d \uad6c\uc131\ub418\uc5b4\uc788\uc74c 1) Extractive Summarization Output (Train/Test\ub3d9\uc77c) batch = [ torch.tensor(src_doc_padded), torch.tensor(src_doc_mask), torch.tensor(src_doc_type_padded), torch.tensor(cls_index), torch.tensor(ext_labels), src_summary, tgt_doc_summary ] \uc544\ub798 \uc138 \uc885\ub958(src / cls, ext / summary)\uc758 \ub370\uc774\ud130\ub294 BERT\uc758 input\uc73c\ub85c \ud65c\uc6a9 src_doc_padded : padding\uc744 \uc644\ub8cc\ud55c source document\uc758 sequence src_doc_mask : source document\uc758 mask sequence src_doc_type_padded : source document \ubb38\uc7a5\uc758 token type sequence cls_index : source document\uc5d0\uc11c cls token\uc758 index ext_labels : Extractive summarization label src_summary, tgt_doc_summary : \uc6d0\ubcf8 \ud14d\uc2a4\ud2b8 2-1) Abstractive Summarization Output (Train) batch = [ torch.tensor(src_doc_padded), torch.tensor(src_doc_mask), torch.tensor(src_doc_type_padded), torch.tensor(tgt_doc_padded), torch.tensor(tgt_doc_mask), ] \uc544\ub798 \uc138\uac00\uc9c0 \ub370\uc774\ud130\ub294 BERT (encoder) \uc758 input\uc73c\ub85c \ud65c\uc6a9 src_doc_padded : padding\uc744 \uc644\ub8cc\ud55c source document\uc758 sequence src_doc_mask : source document\uc758 mask sequence src_doc_type_padded : source document \ubb38\uc7a5\uc758 token type sequence \uc544\ub798 \ub450\uac00\uc9c0 \ub370\uc774\ud130\ub294 decoder \uc758 input\uc73c\ub85c \ud65c\uc6a9 tgt_doc_padded : padding\uc744 \uc644\ub8cc\ud55c target document\uc758 sequence tgt_doc_mask : target document\uc758 mask sequence 2-2) Abstractive summarization output (Test) batch = [ torch.tensor(src_doc_padded), torch.tensor(src_doc_mask), torch.tensor(src_doc_type_padded), torch.tensor(tgt_doc_padded), torch.tensor(tgt_doc_mask), tgt_doc_summary ] \uc544\ub798 \uc138\uac00\uc9c0 \ub370\uc774\ud130\ub294 BERT (encoder) \uc758 input\uc73c\ub85c \ud65c\uc6a9 src_doc_padded : padding\uc744 \uc644\ub8cc\ud55c source document\uc758 sequence src_doc_mask : source document\uc758 mask sequence src_doc_type_padded : source document \ubb38\uc7a5\uc758 token type sequence \uc544\ub798 \ub450\uac00\uc9c0 \ub370\uc774\ud130\ub294 test \uacfc\uc815\uc5d0\uc11c \uc2e4\uc81c\ub85c \uc0ac\uc6a9\ub418\uc9c0\uc54a\uc74c tgt_doc_padded : padding\uc744 \uc644\ub8cc\ud55c target document\uc758 sequence tgt_doc_mask : target document\uc758 mask sequence tgt_doc_summary : \uc6d0\ubcf8 \ud14d\uc2a4\ud2b8 2) greedy.py Extractive Summarization\uc744 \uc9c4\ud589\ud558\uae30 \uc704\ud574\uc120 Extractive Summary Sentences\uac00 \ud544\uc694\ud558\uba70, \uc774\ub294 \ubcf4\ud3b8\uc801\uc73c\ub85c \uc6d0\ubb38\uacfc Gold summary(abstractive)\uc758 Greedy Selection \uc744 \ud1b5\ud574 \uc774\ub8e8\uc5b4\uc9c4\ub2e4. Greedy Selection \uc740 \uc6d0\ubb38\uc758 \ubb38\uc7a5\ub4e4\uc744 \uc21c\ud68c\ud558\uba70 Gold summary\uc640\uc758 ROUGE-2 score\ub97c \uacc4\uc0b0\ud55c \ub4a4 \uc21c\uc704\ub97c \ub9e4\uae34\ub2e4. \uadf8 \ud6c4 \uc21c\uc704\uac00 \ub192\uc740 \uc21c\uc73c\ub85c \uae30\uc900 \uac1c\uc218 \ub9cc\ud07c \uc120\ud0dd\ud558\uba70, BertSum\uc5d0\uc11c\ub294 top-3 \ubb38\uc7a5\uc744 \uc120\ud0dd\ud588\ub2e4. def _get_ngrams(n, text) Parameters n: \uba87\uac1c\uc758 token\uc744 \ucd94\ucd9c\ud55c \uac83\uc778\uc9c0\uc5d0 \ub300\ud55c \ud30c\ub77c\ubbf8\ud130 ( n -gram) text: ngram \ucd94\ucd9c \ub300\uc0c1 \ubb38\uc7a5 def _get_word_ngrams(n, sentences) Parameters n: \uba87\uac1c\uc758 token\uc744 \ucd94\ucd9c\ud55c \uac83\uc778\uc9c0\uc5d0 \ub300\ud55c \ud30c\ub77c\ubbf8\ud130 ( n -gram) sentences: \uba87 \uac1c \ubb38\uc7a5\uc744 \ub300\uc0c1\uc73c\ub85c ngram\uc744 \uad6c\ud560 \uc9c0\uc5d0 \ub300\ud55c \ud30c\ub77c\ubbf8\ud130 def cal_rouge(evaluated_ngrams, reference_ngrams) Parameters evaluated_ngrams: \uc6d0\ubb38 \ubb38\uc7a5 tokens reference_ngrams: \uc0dd\uc131 \uc694\uc57d \ubb38\uc7a5 tokens def greedy_selection(doc_sent_list, abstract_sent_list, summary_size) Parameters doc_sent_list: \uc6d0\ubb38 \ubb38\uc7a5\ubcc4 \ubd84\ud560 \ub9ac\uc2a4\ud2b8 abstract_sent_list: \uc0dd\uc131\uc694\uc57d \ubb38\uc7a5\ubcc4 \ubd84\ud560 \ub9ac\uc2a4\ud2b8 summary_size: \ucd94\ucd9c \uc694\uc57d \ubb38\uc7a5 \uac2f\uc218 def greedy_extract(doc_sent_list, abstract_sent_list, summary_size): Parameters doc_sent_list: \uc6d0\ubb38 \ubb38\uc7a5\ubcc4 \ubd84\ud560 \ub9ac\uc2a4\ud2b8 abstract_sent_list: \uc0dd\uc131\uc694\uc57d \ubb38\uc7a5\ubcc4 \ubd84\ud560 \ub9ac\uc2a4\ud2b8 summary_size: \ucd94\ucd9c \uc694\uc57d \ubb38\uc7a5 \uac2f\uc218 3) modeling_utils.py def set_seed args modeling\uc5d0 \uc0ac\uc6a9\ub418\ub294 \ubaa8\ub4e0 \ub79c\ub364\uc2dc\ub4dc\ub97c \uace0\uc815\ud558\ub294 \ud568\uc218 def save_pkl path , file pickle \ud30c\uc77c\uc744 \uc800\uc7a5\ud558\ub294 \ud568\uc218 def load_pkl path , file pickle \ud30c\uc77c\uc744 \ub85c\ub4dc\ud558\ub294 \ud568\uc218 4) preprocess.py Class Korean_Dataset Train, Valid, Test\uc5d0 \ub9de\uac8c \ub370\uc774\ud130\uc14b\uc744 \uc804\ucc98\ub9ac \ud558\ub294 \ud074\ub798\uc2a4. def parse_data data_type , save_path \uc6d0\ubcf8 \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc628 \ub2e4\uc74c korean tokenizer\ub97c \ud65c\uc6a9\ud558\uc5ec \ub370\uc774\ud130\ub97c token\ud654 \ud558\ub294 \ud568\uc218. output\uc73c\ub85c\ub294 pickle \ud615\ud0dc\uc758 \ud30c\uc77c\uc774 \uc800\uc7a5 \uc800\uc7a5\ub41c \ud30c\uc77c\uc740 dictionary \ud615\ud0dc\ub85c 5\uac00\uc9c0 key\ub97c \uac00\uc9c4\ub2e4 src_tokens : tokenize\ub41c source document tgt_tokens : tokenize\ub41c target docuement src_raw : \uc6d0\ubcf8 source document tgt_raw : \uc6d0\ubcf8 target docuement ext_labels : greedy selection\uc744 \uc9c4\ud589\ud55c extractive summarization label 4. Ext - Extractive Summarization \uc0ac\uc6a9\ub418\ub294 \ucf54\ub4dc\ub294 \ud06c\uac8c 4\uac1c\ub85c \ub2e4\uc74c\uacfc \uac19\uc740 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uac16\uace0 \uc788\ub2e4. modeling_bertext: Extractive Summarization Model ext_trainer / ext_Train: \ud559\uc2b5 \uacfc\uc815\uc5d0 \ud544\uc694\ud55c \uc804\ubc18\uc801\uc778 \ucf54\ub4dc ext_test: \ucd94\ub860 \uacfc\uc815\uacfc ROUGE Score \uacc4\uc0b0\uc5d0 \ud544\uc694\ud55c \ucf54\ub4dc 1) modeling_bertext.py class BertExt max_pos : int, ext_layers : int, ext_heads : int, ext_ff_size : int, ext_dropout : int, param_init_glorot = True, language = 'kor' BertSum Extractive Model\uc744 \uad6c\uc131\ud558\ub294 Class\uc774\ub2e4. language \uc5d0 \ub530\ub77c \uad6c\uc131 \ubaa8\ub378\uc758 \uc5b8\uc5b4\uac00 \ubc14\ub00c\uc5b4 \uc0ac\uc6a9\ud558\ub294 Bert\uac00 \ub2ec\ub77c\uc9c0\uba70, \ub098\uba38\uc9c0 \ud30c\ub77c\ubbf8\ud130\ub4e4\uc740 BertSum Extractive Model\uc5d0 \ud3ec\ud568\ub418\ub294 Transformer\uc758 \uad6c\uc870\ub97c \ub2e4\uc591\ud558\uac8c \uc870\uc808\ud558\ub294 \ud30c\ub77c\ubbf8\ud130\ub4e4\uc774\ub2e4. Parameters max_pos: \ubb38\uc7a5\ubcc4 \ucd5c\ub300 \uae38\uc774 ext_layers: Extractive Transformer Encoder Layer \uac1c\uc218 ext_heads: MultiHeadAttention Parallel Heads \uac1c\uc218 ext_ff_size: PositionwiseFeadForward Hidden Layers \uac2f\uc218 ext_dropout: PositionwiseFeadForward Dropout Rate param_init_glorot: Layer\ubcc4 Xavier_uniform \uc801\uc6a9 \uc5ec\ubd80 language( str , optional, default to kor ) - BERT \ub610\ub294 KoBERT \uc0ac\uc6a9 \uc720\ubb34 def forward encoder_input_ids, encoder_attention_mask, token_type_ids, cls_index, cls_mask, ext_labels BERT(\ub610\ub294 KoBERT)\uc5d0 token id(encoder_input_ids) / attention mask(encoder_attention_mask) / \ubb38\uc7a5 \uad6c\ubd84 id (token_type_ids)\ub97c \uc785\ub825\uc73c\ub85c \ubc1b\uc544, \ubb38\uc7a5\uc758 BERT Encoding \uac12\uc744 top_vec\uc73c\ub85c return \ubc1b\uace0, CLS Token\uc758 \uac12\ub9cc\uc744 \ud544\ud130\ub9c1\ud558\uc5ec Transformer Layer\ub97c \ud1b5\uacfc\ud574 \ubb38\uc7a5\ubcc4 \uc810\uc218\ub97c Return\ud569\ub2c8\ub2e4. class PositionwiseFeedForward head_count, model_dim, dropout=0.1, use_final_linear=True Transformer\uc758 PositionwiseFeedForward Function (\uc800\uc790\ucf54\ub4dc) class MultiHeadedAttention Transformer\uc758 MultiHeadedAttention class Classifier hidden_size BERT\uc758 Last Layer\ub97c \ud1b5\ud574 \ubd84\ub958 \ubb38\uc81c \ud574\uacb0 Function class PositionalEncoding dropout, dim, max_len-5000 Transformer\uc758 PositionalEncoding class TransformerEncoderLayer d_model, heads, d_ff, dropout Transformer\uc758 EncodingLayer\ub97c \uad6c\ud604\ud55c \uac83\uc778\ub370, \uc774\ub294 HuggingFace\uc5d0\uc11c \uc9c0\uc6d0\ud558\uc9c0 \uc54a\uc73c\ubbc0\ub85c \uad6c\ud604 (\uc800\uc790\ucf54\ub4dc) class ExtTransformerEncoder d_model, d_ff, heads, dropout, num_inter_layers =0 TransformerEncoderLayer\ub97c \uc8fc\uc5b4\uc9c4 \ud69f\uc218\ub9cc\ud07c \ubc18\ubcf5\ud558\ub294 class 2) ext_train.py \ubaa8\ub378\uacfc \ub370\uc774\ud130\uc14b\uc744 \ud1b5\ud574 train\uc744 \uc9c4\ud589\ud558\uba70, \uc2e4\uc9c8\uc801 \ud6c8\ub828\uc5d0 \ub300\ud55c \ucf54\ub4dc\ub294 ext_trainer.py\uc758 train\uc5d0 \uc815\uc758\ub418\uc5b4 \uc788\ub2e4. def main args: local_rank, train_path, val_path, max_len, language, ext_layers, ext_heades, ext_ff_size, ext_dropout, param_init_glorot, device Parameters args \u200b BertExt \ubaa8\ub378 \uad6c\uc131 \ud30c\ub77c\ubbf8\ud130 max_len ext_heads ext_ff_size ext_dropout ext_layers param_init_glorot language train_path / val_path: Train Data / Validation Data \uacbd\ub85c max_len: \uc694\uc57d \ub300\uc0c1 \ucd5c\ub300 \uae38\uc774 language: \uc5b8\uc5b4 3) ext_trainer.py class SequentialDistributedSampler (Sampler): Sampler\ub294 Index\ub97c \ub2e4\ub8e8\ub294 \ubc29\ubc95\uc774\uba70, \ub370\uc774\ud130\uc758 Index\ub97c \uc6d0\ud558\ub294 \ubc29\uc2dd\ub300\ub85c \uc870\uc815\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. Sequential\ud55c \uc21c\uc11c\ub85c DataLoader\ub97c \uad6c\uc131\ud558\uba70, Distributed Setting\uc5d0 \uac78\ub9de\uac8c \uc9c4\ud589\ud569\ub2c8\ub2e4. def train args, model, train_dataset, eval_dataset trainer\uc758 train \ud568\uc218 \uacfc\uc815\uc740 train_dataset\uc744 \ubc30\uce58\ub2e8\uc704\ub85c \uc21c\ud68c\ud558\uba74\uc11c Model\uc758 Input\uc73c\ub85c \uc81c\uacf5\ud55c\ub2e4. \uc774\ud6c4 backward, step\uacfc \uac19\uc740 \uc77c\ubc18\uc801\uc778 Update\uacfc\uc815\uc744 \uc9c4\ud589\ud558\uace0 \ud6c8\ub828\uc774 \uc644\ub8cc\uac00 \ub418\uba74 train.py\uc5d0\uc11c global_step, tr_loss, best_val_loss, best_step = train(args, model, train_dataset, val_dataset,) \uc640 \uac19\uc740 \uaf34\ub85c \uc0ac\uc6a9\ud55c\ub2e4. Parameters args train_batch_size train_batch_size_per_gpu val_batch_size gpus language local_rank num_workers max_steps num_train_epochs gradient_accumulation_steps: gradient accumulation \ud69f\uc218 warmup_steps warmup_percent lr fp16 max_grad_norm description model: \ud6c8\ub828\ud558\uace0\uc790 \ud558\ub294 Model train_dataset eval_dataset def evaluate args, model, eval_datasets, prefix=\"\" train\uacfc \uac19\uc740 \uaf34\uc758 \ud568\uc218 \uad6c\uc131\uc744 \uac16\uc9c0\ub9cc update\uacfc\uc815 \uc5c6\uc774 loss\ub9cc return\ud55c\ub2e4. Parameters args checkpoint_dir local_rank val_batch_size val_batch_size_per_gpu gpus num_workers language device model eval_datasets prefix 4) ext_test.py Test Data\ub97c Model Checkpoint\ub85c \uad6c\uc131\ud55c \ubaa8\ub378\uc758 Input\uc73c\ub85c \ub123\uc5b4 Inference\ub97c \uc9c4\ud589\ud558\uc5ec, ROUGE Score\ub97c \uacc4\uc0b0\ud574\ub0b8\ub2e4. def main args: test_data_path, save_path, device, ext_layers, ext_heads, ext_ff_size, ext_dropout, param_init_glorot, max_len, language, seed, local_rank, checkpoint_dir, val_batch_size BertExt \ubaa8\ub378 \uad6c\uc131 \ud30c\ub77c\ubbf8\ud130 max_len ext_heads ext_ff_size ext_dropout ext_layers param_init_glorot language path \uad00\ub828 \ud30c\ub77c\ubbf8\ud130 test_data_path save_path: format_rouge_scores return \uac12 \uc800\uc7a5 \uc704\uce58 checkpoint_dir: model ckpt \uc704\uce58 test_dataloader \uad00\ub828 \ud30c\ub9c8\ud2f0\ub7ec val_batch_size def format_rouge_scores scores Rouge-(1,2,L) \uc810\uc218\ub97c \ubaa8\ub450 Dictionary\ub85c \uac16\uace0 \uc788\ub294 scores\ub97c Input\uc73c\ub85c \ubc1b\uc544, \uc544\ub798\uc758 \uc608\uc2dc\uc640 \uac19\uc740 \uaf34\ub85c Return def save_rouge_scores str_scores format_rouge_scores\ub85c ROUGE SCORE\ub97c Formatting\ud55c \uacb0\uacfc\ub97c \ubc1b\uc544 txt file\ub85c \uc800\uc7a5 5. Abs - Abstractive Summarization \uc0ac\uc6a9\ub418\ub294 \ucf54\ub4dc\ub294 \ud06c\uac8c 5\uac1c\ub85c \ub2e4\uc74c\uacfc \uac19\uc740 \uad6c\uc870\ub97c \uac00\uc9c0\uace0\uc788\uc73c\uba70 modeling_bertabs\ub294 abstractive summarization\uc744 \uc704\ud55c \ubaa8\ub378\uc774, abs_trainer\uc640 abs_train\uc740 \ud559\uc2b5 \uacfc\uc815\uc5d0 \ud544\uc694\ud55c \uc804\ubc18\uc801\uc778 \ucf54\ub4dc\ub97c, abs_generate\uc640 abs_inference\ub294 \ucd94\ub860 \uacfc\uc815\uc5d0\uc11c \ud544\uc694\ud55c \uc804\ubc18\uc801\uc778 \ucf54\ub4dc\ub97c \ub098\ud0c0\ub0b8\ub2e4. 1) modeling_bertabs.py Abstractive \ub97c \uc704\ud55c \ubaa8\ub378\uc740 BertAbs class \ub0b4\ubd80\uc5d0 \uc815\uc758\ub418\uc5b4\uc788\uc73c\uba70 \ubaa8\ub378\uc758 \uad6c\uc870\ub294 Presumm \uc800\uc790\ucf54\ub4dc\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub418 \ud55c\uad6d\uc5b4 \ubaa8\ub378\uc744 \uad6c\ud604\ud558\ub294 \uacfc\uc815\uc5d0\uc11c \uba87\uac00\uc9c0 \ubd80\ubd84\uc744 \uc218\uc815\ud558\uc600\uc74c def get_kobert_vocab \uc544\ub798\uc640 \uac19\uc740 \ucf54\ub4dc\ub85c \ubaa8\ub378\uacfc vocab\uc744 \ud568\uaed8 \ubd88\ub7ec\uc624\ub294\uac83\uc774 \uc77c\ubc18\uc801\uc774\ub098, decoder part\uc5d0\uc11c BOS, EOS token\uc744 \ucd94\uac00\ud574\uc8fc\uc5b4\uc57c\ud558\uae30\ub54c\ubb38\uc5d0 vocab\uc744 \ub530\ub85c \ubd88\ub7ec\uc624\ub294 \ud568\uc218\ub97c \uc7ac \uc815\uc758\ud568 bert, vocab = get_pytorch_kobert_model() \ud568\uc218\uc5d0\uc11c \uc544\ub798\uc640 \uac19\uc740 \ucf54\ub4dc\ub97c \ud1b5\ud574 \uac04\ub2e8\ud788 \ub450\uac1c\uc758 token\uc744 \ucd94\uac00\ud560 \uc218 \uc788\uc73c\uba70, gluonnlp\uc758 https://nlp.gluon.ai/_modules/gluonnlp/vocab/bert.html \ub9c1\ud06c\ub97c \ucc38\uc870 \ud558\uc600\uc74c vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece( vocab_file, padding_token=\"[PAD]\", bos_token=\"[BOS]\", eos_token=\"[EOS]\" ) Class BertAbs input max_pos dec_layers dec_hidden_size dec_heads dec_ff_size dec_dropout language : Korean checkpoint_dir : pretrained Encoder\ub97c \ubd88\ub7ec\uc62c \ub54c \uc0ac\uc6a9 def forward encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask Dataset\uc73c\ub85c\ubd80\ud130 \ub098\uc624\ub294 src_doc_padded = encoder_input_ids, tgt_doc_padded = decoder_input_ids, src_doc_type_padded = token_type_ids, src_doc_mask = encoder_attention_mask\uc5d0 \uac01\uac01 \ub9e4\uce6d\ub41c\ub2e4 1) Encoder bert \ubaa8\ub378\uc744 \ud1b5\uacfc\ud558\uc5ec \uc5bb\uc5b4\uc9c4 encoder_output \uc73c\ub85c \ubd80\ud130 encoder_hidden_states \ub97c \uc5bb\uc744\uc218 \uc788\ub2e4 2) Decoder 2-1) Decoder initial state encoder_input_ids`` , encoder_hidden_states`\ub97c \ud65c\uc6a9\ud558\uc5ec decoder\uc758 initial state\ub97c \uc5bb\uace0 2-2) Decoder output decoder_input_ids , encoder_hidden_states , dec_state \ub97c \uc0ac\uc6a9\ud558\uc5ec decoder_outputs \ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4 2-3) Generator decoder_output \uc744 \ud65c\uc6a9\ud558\uc5ec Linear layer\ub97c \uac70\uccd0 hidden vector\ub97c vocab\uc73c\ub85c \ubcc0\ud658\ud558\ub294 \uacfc\uc815\uc744 \uac70\uce5c\ub2e4 Class TransformerDecoder \ubcf8 \ucf54\ub4dc\ub294 \"Attention is All You Need\" \ub17c\ubb38\uc758 \ucf54\ub4dc\ub97c \uadf8\ub300\ub85c \uac00\uc838\uc640 \uc0ac\uc6a9\ud558\uc600\uc73c\uba70 \ubcf8 \ucf54\ub4dc\ub294 scratch\ubd80\ud130 transformer decoder\ub97c \uad6c\ud604\ud55c \ucf54\ub4dc\uc774\ub2e4. https://arxiv.org/abs/1706.03762\ub97c \ud1b5\ud574 \uad6c\uc870\ub97c \uc774\ud574\ud560 \uc218 \uc788\ub2e4.","title":"Presumm"},{"location":"presumm/#presumm","text":"","title":"Presumm"},{"location":"presumm/#1-overview","text":"Presumm\uc740 Bertsum\uc758 \uc800\uc790\uac00 \ubc1c\ud45c\ud55c \ud6c4\uc18d \ub17c\ubb38\uc73c\ub85c, Extractive summarization\uc744 \ubaa9\uc801\uc73c\ub85c \ud55c Bertsum\uc744 \ud65c\uc6a9\ud558\uc5ec Extractive summarization model \uacfc Abstractive summarization \uc774 \ubaa8\ub450 \uac00\ub2a5\ud55c \ub450\uac00\uc9c0 \ubaa8\ub378\uc744 \uc81c\uc548\ud55c\ub2e4.","title":"1. Overview"},{"location":"presumm/#1-presumm-for-extractive-summarization","text":"\uae30\uc874\uc758 BERT input\uacfc \ub2e4\ub974\uac8c \ubaa8\ub4e0 \ubb38\uc7a5 \uc55e\ub4a4\uc5d0 [CLS], [SEP] token\uc744 \ubc30\uce58\ud568 \ud55c\uad6d\uc5b4 \ubaa8\ub378\uc744 \uad6c\ud604\ud558\uae30\uc704\ud574 SKT\uc5d0\uc11c \uacf5\uac1c\ud55c KoBERT \ubaa8\ub378\uc744 \ud65c\uc6a9\ud568 KoBERT\uc758 \uc544\uc6c3\ud48b \ubca1\ud130\ub97c \ubaa8\ub450 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 CLS token\uc758 Top vector\ub9cc \uc0ac\uc6a9\ud558\uc5ec Transformer Encoder Layer\ub97c \ud1b5\uacfc\ud568 (\uc0ac\uc6a9\ud55c \ub808\uc774\uc5b4 \uac2f\uc218\ub294 2\uac1c)","title":"1) Presumm for Extractive summarization"},{"location":"presumm/#2-presumm-for-abstractive-summarization","text":"Encoder\ub294 KoBERT \ud639\uc740 Pretrained Extractive Summarization Model\uc744 \uc0ac\uc6a9\ud558\uace0, Decoder\ub294 Transformer Decoder\ub97c \uc0ac\uc6a9\ud558\ub294 \ubaa8\ub378 \uad6c\uc870 Encoder\uc758 Input\uc740 Extractive model\uacfc \ub3d9\uc77c\ud558\uac8c \ud558\ub098\uc758 \ubb38\uc7a5 \uc55e\ub4a4\uc5d0 [CLS], [SEP] token\uc744 \ubd99\uc784 Decoder\ub294 \ubcc4\ub3c4\uc758 Special token\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc9c0\ub9cc, \ubb38\uc7a5 \uc55e\ub4a4\uc5d0 [BOS], [EOS] \ub97c \ub354\ud558\uc5ec Document\uc758 \uc2dc\uc791\uacfc \ub05d\uc744 \ud45c\uc2dc\ud568 Extractive model\uacfc \ub2e4\ub974\uac8c Document \uc804\uccb4 \ubca1\ud130\ub97c \ubaa8\ub450 \ud65c\uc6a9\ud558\uc5ec Decoder\uc5d0 \ub123\uc5b4\uc90c Beam search\ub97c \ud65c\uc6a9\ud558\uc5ec \ucd5c\uc885 Output vocab\uc744 \ucc3e\uc544\ub0c4","title":"2) Presumm for Abstractive summarization"},{"location":"presumm/#2-code-scheme","text":"Presumm \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 utils \u2502 \u251c\u2500\u2500 dataset.py \u2502 \u251c\u2500\u2500 greedy.py \u2502 \u251c\u2500\u2500 modeling_utils.py \u2502 \u2514\u2500\u2500 preprocess.py \u251c\u2500\u2500 ext \u2502 \u251c\u2500\u2500 modeling_bertext.py \u2502 \u251c\u2500\u2500 ext_train.py \u2502 \u251c\u2500\u2500 ext_trainer.py \u2502 \u251c\u2500\u2500 inference.py \u2502 \u2514\u2500\u2500 ext_test.py \u251c\u2500\u2500 abs \u2502 \u251c\u2500\u2500 modeling_bertabs.py \u2502 \u251c\u2500\u2500 abs_train.py \u2502 \u251c\u2500\u2500 abs_trainer.py \u2502 \u251c\u2500\u2500 abs_test.py \u2502 \u251c\u2500\u2500 abs_generate.py \u2502 \u2514\u2500\u2500 train.sh","title":"2. Code scheme"},{"location":"presumm/#3-utils","text":"","title":"3. Utils"},{"location":"presumm/#1-datasetpy","text":"Class Bertsum_Dataset data_path : str, max_len = 512 , train = True, mode = 'ext', language = 'kor' Bertsum\uc5d0 \uc0ac\uc6a9\ub420 \ub370\uc774\ud130\uc14b class\uc774\uba70, Extractive summarization\uacfc Abstract summarization \ubaa8\ub450 \uc801\uc6a9 \uac00\ub2a5\ud55c \ud568\uc218\ub85c mode \ub97c \uc815\ud655\ud788 \ubc14\uafb8\uc5b4\uc8fc\uc5b4\uc57c\ud55c\ub2e4. \ub610\ud55c \ud559\uc2b5/\ucd94\ub860 \uc5ec\ubd80\uc5d0 \ub530\ub77c Train option\uc744 \ubc14\uafb8\uc5b4\uc11c \uc0ac\uc6a9\ud574\uc57c\ud558\uba70 Dataloader\ub97c \uc0ac\uc6a9\ud560\ub54c Dataset \uc635\uc158\uc5d0 \ud574\ub2f9 class\ub97c import\ud558\uace0 \uc0ac\uc6a9\ud574\uc57c\ud55c\ub2e4. Parameters data_path ( str , require) max_len ( int ,optional, defaults to 512 ) train ( bool , optional, defaults to True ) mode ( str ) if string, ext , abs are supported language ( str , defaults to 'kor') def Collate batch Collate \ud568\uc218\ub294 \ubcf4\ud1b5 Data Loader\uc758 \ubc30\uce58 \ub0b4 \ub370\uc774\ud130\ub4e4\uc774 \uae38\uc774\ub098 \uad6c\uc131\uc744 \uac19\uac8c \ud574\uc8fc\ub294 \ud568\uc218\uc774\ub2e4. \ud574\ub2f9 Collate Function\uc774 \uc9c4\ud589\ud558\ub294 \ucc98\ub9ac\ub294 \ub370\uc774\ud130\uac04 \uae38\uc774\ub97c \ub9de\ucdb0\uc8fc\uae30 \uc704\ud574 \ud328\ub529\uc744 \uc2e4\uc2dc\ud574\uc57c \ud558\ub294\ub370 \ubb38\uc7a5\uc774 Max Length\ub97c \ub118\uc5b4\uac08 \uacbd\uc6b0, \ud574\ub2f9 Max Length \uc774\ud558\uc758 \ub9c8\uc9c0\ub9c9 [CLS] Token\uc758 \uc704\uce58\ub97c \ucc3e\uc544 \ubb38\uc7a5 \ud328\ub529\uc744 \uc9c4\ud589\ud55c\ub2e4. def src_add_pad token_ids, token_type token_ids \ub294 \uc804\uccb4 document\ub97c token\ud654 \ud55c \ud6c4 vocab\uc5d0 \ub530\ub77c id\ub85c \ubcc0\uacbd\ud55c \uac83\uc744 \uc758\ubbf8\ud558\uba70, token_type \uc740 document\uc758 \ubb38\uc7a5\uc744 \uad6c\ubcc4\ud574\uc8fc\ub294 \uc2dd\ubcc4\uc790\uc774\ub2e4. \ud574\ub2f9 \ud568\uc218\ub294 source document\ub97c max_length\uc5d0 \ub530\ub77c padding\uc744 \uc9c4\ud589\ud574\uc8fc\ub294 \ud568\uc218 def tgt_add_pad token_ids \ud574\ub2f9 \ud568\uc218\ub294 target document\ub97c max_length\uc5d0 \ub530\ub77c padding\uc744 \uc9c4\ud589\ud574\uc8fc\ub294 \ud568\uc218 def add_special_token token_ids \ud574\ub2f9 \ud568\uc218\ub294 source document\uc5d0\uc11c \ud55c \ubb38\uc7a5\ub9c8\ub2e4 special token\uc778 [CLS]\uc640 [SEP]\ub85c \uac10\uc2f8\uc8fc\ub294 \ud568\uc218 def add_sentence_token token_ids \ud574\ub2f9 \ud568\uc218\ub294 target document \uc804\uccb4\ub97c [BOS], [EOS]\ub85c \uac10\uc2f8\uc8fc\ub294 \uc5ed\ud560\uc744 \ud558\uba70 \ubb38\uc7a5\ub2e8\uc704\uac00 \uc544\ub2c8\ub77c \uc804\uccb4 document\ub97c \uac10\uc2f8\uc8fc\ub294 \ud568\uc218 def idx2mask token_ids padding index\ub85c \uc774\ub8e8\uc5b4\uc9c4 sequence\ub97c mask\ub85c \ubc14\uafb8\uc5b4\uc8fc\ub294 \ud568\uc218 def get_token_type_ids src_token_padded source document\uc758 seuquence\uac00 \uc874\uc7ac\ud560 \ub54c \ubb38\uc7a5 \ub2e8\uc704\ub85c \uad6c\ubcc4\ud558\uae30 \uc704\ud574 type id\ub97c \ubd80\uc5ec\ud558\ub294 \ud568\uc218 def get_cls_index src_token_padded Extractive summarization\uc5d0\uc11c\ub9cc \uc0ac\uc6a9\ub418\ub294 \ud568\uc218\ub85c cls token\uc758 \uc704\uce58\ub97c \uc54c\uc544\ub0b4\uae30\uc704\ud574 cls index \uc704\uce58\ub97c \ucc3e\ub294 \ud568\uc218 def _ getitem _ idx Pytorch Dataset\uc744 \ub9cc\ub4e4\uae30\uc704\ud55c \ud568\uc218\ub85c, \ud559\uc2b5\uacfc \ucd94\ub860\uc2dc batch \ub2e8\uc704\ub85c \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc62c \uc218 \uc788\ub3c4\ub85d \uad6c\uc131\ub418\uc5b4\uc788\uc74c 1) Extractive Summarization Output (Train/Test\ub3d9\uc77c) batch = [ torch.tensor(src_doc_padded), torch.tensor(src_doc_mask), torch.tensor(src_doc_type_padded), torch.tensor(cls_index), torch.tensor(ext_labels), src_summary, tgt_doc_summary ] \uc544\ub798 \uc138 \uc885\ub958(src / cls, ext / summary)\uc758 \ub370\uc774\ud130\ub294 BERT\uc758 input\uc73c\ub85c \ud65c\uc6a9 src_doc_padded : padding\uc744 \uc644\ub8cc\ud55c source document\uc758 sequence src_doc_mask : source document\uc758 mask sequence src_doc_type_padded : source document \ubb38\uc7a5\uc758 token type sequence cls_index : source document\uc5d0\uc11c cls token\uc758 index ext_labels : Extractive summarization label src_summary, tgt_doc_summary : \uc6d0\ubcf8 \ud14d\uc2a4\ud2b8 2-1) Abstractive Summarization Output (Train) batch = [ torch.tensor(src_doc_padded), torch.tensor(src_doc_mask), torch.tensor(src_doc_type_padded), torch.tensor(tgt_doc_padded), torch.tensor(tgt_doc_mask), ] \uc544\ub798 \uc138\uac00\uc9c0 \ub370\uc774\ud130\ub294 BERT (encoder) \uc758 input\uc73c\ub85c \ud65c\uc6a9 src_doc_padded : padding\uc744 \uc644\ub8cc\ud55c source document\uc758 sequence src_doc_mask : source document\uc758 mask sequence src_doc_type_padded : source document \ubb38\uc7a5\uc758 token type sequence \uc544\ub798 \ub450\uac00\uc9c0 \ub370\uc774\ud130\ub294 decoder \uc758 input\uc73c\ub85c \ud65c\uc6a9 tgt_doc_padded : padding\uc744 \uc644\ub8cc\ud55c target document\uc758 sequence tgt_doc_mask : target document\uc758 mask sequence 2-2) Abstractive summarization output (Test) batch = [ torch.tensor(src_doc_padded), torch.tensor(src_doc_mask), torch.tensor(src_doc_type_padded), torch.tensor(tgt_doc_padded), torch.tensor(tgt_doc_mask), tgt_doc_summary ] \uc544\ub798 \uc138\uac00\uc9c0 \ub370\uc774\ud130\ub294 BERT (encoder) \uc758 input\uc73c\ub85c \ud65c\uc6a9 src_doc_padded : padding\uc744 \uc644\ub8cc\ud55c source document\uc758 sequence src_doc_mask : source document\uc758 mask sequence src_doc_type_padded : source document \ubb38\uc7a5\uc758 token type sequence \uc544\ub798 \ub450\uac00\uc9c0 \ub370\uc774\ud130\ub294 test \uacfc\uc815\uc5d0\uc11c \uc2e4\uc81c\ub85c \uc0ac\uc6a9\ub418\uc9c0\uc54a\uc74c tgt_doc_padded : padding\uc744 \uc644\ub8cc\ud55c target document\uc758 sequence tgt_doc_mask : target document\uc758 mask sequence tgt_doc_summary : \uc6d0\ubcf8 \ud14d\uc2a4\ud2b8","title":"1) dataset.py"},{"location":"presumm/#2-greedypy","text":"Extractive Summarization\uc744 \uc9c4\ud589\ud558\uae30 \uc704\ud574\uc120 Extractive Summary Sentences\uac00 \ud544\uc694\ud558\uba70, \uc774\ub294 \ubcf4\ud3b8\uc801\uc73c\ub85c \uc6d0\ubb38\uacfc Gold summary(abstractive)\uc758 Greedy Selection \uc744 \ud1b5\ud574 \uc774\ub8e8\uc5b4\uc9c4\ub2e4. Greedy Selection \uc740 \uc6d0\ubb38\uc758 \ubb38\uc7a5\ub4e4\uc744 \uc21c\ud68c\ud558\uba70 Gold summary\uc640\uc758 ROUGE-2 score\ub97c \uacc4\uc0b0\ud55c \ub4a4 \uc21c\uc704\ub97c \ub9e4\uae34\ub2e4. \uadf8 \ud6c4 \uc21c\uc704\uac00 \ub192\uc740 \uc21c\uc73c\ub85c \uae30\uc900 \uac1c\uc218 \ub9cc\ud07c \uc120\ud0dd\ud558\uba70, BertSum\uc5d0\uc11c\ub294 top-3 \ubb38\uc7a5\uc744 \uc120\ud0dd\ud588\ub2e4. def _get_ngrams(n, text) Parameters n: \uba87\uac1c\uc758 token\uc744 \ucd94\ucd9c\ud55c \uac83\uc778\uc9c0\uc5d0 \ub300\ud55c \ud30c\ub77c\ubbf8\ud130 ( n -gram) text: ngram \ucd94\ucd9c \ub300\uc0c1 \ubb38\uc7a5 def _get_word_ngrams(n, sentences) Parameters n: \uba87\uac1c\uc758 token\uc744 \ucd94\ucd9c\ud55c \uac83\uc778\uc9c0\uc5d0 \ub300\ud55c \ud30c\ub77c\ubbf8\ud130 ( n -gram) sentences: \uba87 \uac1c \ubb38\uc7a5\uc744 \ub300\uc0c1\uc73c\ub85c ngram\uc744 \uad6c\ud560 \uc9c0\uc5d0 \ub300\ud55c \ud30c\ub77c\ubbf8\ud130 def cal_rouge(evaluated_ngrams, reference_ngrams) Parameters evaluated_ngrams: \uc6d0\ubb38 \ubb38\uc7a5 tokens reference_ngrams: \uc0dd\uc131 \uc694\uc57d \ubb38\uc7a5 tokens def greedy_selection(doc_sent_list, abstract_sent_list, summary_size) Parameters doc_sent_list: \uc6d0\ubb38 \ubb38\uc7a5\ubcc4 \ubd84\ud560 \ub9ac\uc2a4\ud2b8 abstract_sent_list: \uc0dd\uc131\uc694\uc57d \ubb38\uc7a5\ubcc4 \ubd84\ud560 \ub9ac\uc2a4\ud2b8 summary_size: \ucd94\ucd9c \uc694\uc57d \ubb38\uc7a5 \uac2f\uc218 def greedy_extract(doc_sent_list, abstract_sent_list, summary_size): Parameters doc_sent_list: \uc6d0\ubb38 \ubb38\uc7a5\ubcc4 \ubd84\ud560 \ub9ac\uc2a4\ud2b8 abstract_sent_list: \uc0dd\uc131\uc694\uc57d \ubb38\uc7a5\ubcc4 \ubd84\ud560 \ub9ac\uc2a4\ud2b8 summary_size: \ucd94\ucd9c \uc694\uc57d \ubb38\uc7a5 \uac2f\uc218","title":"2) greedy.py"},{"location":"presumm/#3-modeling_utilspy","text":"def set_seed args modeling\uc5d0 \uc0ac\uc6a9\ub418\ub294 \ubaa8\ub4e0 \ub79c\ub364\uc2dc\ub4dc\ub97c \uace0\uc815\ud558\ub294 \ud568\uc218 def save_pkl path , file pickle \ud30c\uc77c\uc744 \uc800\uc7a5\ud558\ub294 \ud568\uc218 def load_pkl path , file pickle \ud30c\uc77c\uc744 \ub85c\ub4dc\ud558\ub294 \ud568\uc218","title":"3) modeling_utils.py"},{"location":"presumm/#4-preprocesspy","text":"Class Korean_Dataset Train, Valid, Test\uc5d0 \ub9de\uac8c \ub370\uc774\ud130\uc14b\uc744 \uc804\ucc98\ub9ac \ud558\ub294 \ud074\ub798\uc2a4. def parse_data data_type , save_path \uc6d0\ubcf8 \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc628 \ub2e4\uc74c korean tokenizer\ub97c \ud65c\uc6a9\ud558\uc5ec \ub370\uc774\ud130\ub97c token\ud654 \ud558\ub294 \ud568\uc218. output\uc73c\ub85c\ub294 pickle \ud615\ud0dc\uc758 \ud30c\uc77c\uc774 \uc800\uc7a5 \uc800\uc7a5\ub41c \ud30c\uc77c\uc740 dictionary \ud615\ud0dc\ub85c 5\uac00\uc9c0 key\ub97c \uac00\uc9c4\ub2e4 src_tokens : tokenize\ub41c source document tgt_tokens : tokenize\ub41c target docuement src_raw : \uc6d0\ubcf8 source document tgt_raw : \uc6d0\ubcf8 target docuement ext_labels : greedy selection\uc744 \uc9c4\ud589\ud55c extractive summarization label","title":"4) preprocess.py"},{"location":"presumm/#4-ext-extractive-summarization","text":"\uc0ac\uc6a9\ub418\ub294 \ucf54\ub4dc\ub294 \ud06c\uac8c 4\uac1c\ub85c \ub2e4\uc74c\uacfc \uac19\uc740 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uac16\uace0 \uc788\ub2e4. modeling_bertext: Extractive Summarization Model ext_trainer / ext_Train: \ud559\uc2b5 \uacfc\uc815\uc5d0 \ud544\uc694\ud55c \uc804\ubc18\uc801\uc778 \ucf54\ub4dc ext_test: \ucd94\ub860 \uacfc\uc815\uacfc ROUGE Score \uacc4\uc0b0\uc5d0 \ud544\uc694\ud55c \ucf54\ub4dc","title":"4. Ext - Extractive Summarization"},{"location":"presumm/#1-modeling_bertextpy","text":"class BertExt max_pos : int, ext_layers : int, ext_heads : int, ext_ff_size : int, ext_dropout : int, param_init_glorot = True, language = 'kor' BertSum Extractive Model\uc744 \uad6c\uc131\ud558\ub294 Class\uc774\ub2e4. language \uc5d0 \ub530\ub77c \uad6c\uc131 \ubaa8\ub378\uc758 \uc5b8\uc5b4\uac00 \ubc14\ub00c\uc5b4 \uc0ac\uc6a9\ud558\ub294 Bert\uac00 \ub2ec\ub77c\uc9c0\uba70, \ub098\uba38\uc9c0 \ud30c\ub77c\ubbf8\ud130\ub4e4\uc740 BertSum Extractive Model\uc5d0 \ud3ec\ud568\ub418\ub294 Transformer\uc758 \uad6c\uc870\ub97c \ub2e4\uc591\ud558\uac8c \uc870\uc808\ud558\ub294 \ud30c\ub77c\ubbf8\ud130\ub4e4\uc774\ub2e4. Parameters max_pos: \ubb38\uc7a5\ubcc4 \ucd5c\ub300 \uae38\uc774 ext_layers: Extractive Transformer Encoder Layer \uac1c\uc218 ext_heads: MultiHeadAttention Parallel Heads \uac1c\uc218 ext_ff_size: PositionwiseFeadForward Hidden Layers \uac2f\uc218 ext_dropout: PositionwiseFeadForward Dropout Rate param_init_glorot: Layer\ubcc4 Xavier_uniform \uc801\uc6a9 \uc5ec\ubd80 language( str , optional, default to kor ) - BERT \ub610\ub294 KoBERT \uc0ac\uc6a9 \uc720\ubb34 def forward encoder_input_ids, encoder_attention_mask, token_type_ids, cls_index, cls_mask, ext_labels BERT(\ub610\ub294 KoBERT)\uc5d0 token id(encoder_input_ids) / attention mask(encoder_attention_mask) / \ubb38\uc7a5 \uad6c\ubd84 id (token_type_ids)\ub97c \uc785\ub825\uc73c\ub85c \ubc1b\uc544, \ubb38\uc7a5\uc758 BERT Encoding \uac12\uc744 top_vec\uc73c\ub85c return \ubc1b\uace0, CLS Token\uc758 \uac12\ub9cc\uc744 \ud544\ud130\ub9c1\ud558\uc5ec Transformer Layer\ub97c \ud1b5\uacfc\ud574 \ubb38\uc7a5\ubcc4 \uc810\uc218\ub97c Return\ud569\ub2c8\ub2e4. class PositionwiseFeedForward head_count, model_dim, dropout=0.1, use_final_linear=True Transformer\uc758 PositionwiseFeedForward Function (\uc800\uc790\ucf54\ub4dc) class MultiHeadedAttention Transformer\uc758 MultiHeadedAttention class Classifier hidden_size BERT\uc758 Last Layer\ub97c \ud1b5\ud574 \ubd84\ub958 \ubb38\uc81c \ud574\uacb0 Function class PositionalEncoding dropout, dim, max_len-5000 Transformer\uc758 PositionalEncoding class TransformerEncoderLayer d_model, heads, d_ff, dropout Transformer\uc758 EncodingLayer\ub97c \uad6c\ud604\ud55c \uac83\uc778\ub370, \uc774\ub294 HuggingFace\uc5d0\uc11c \uc9c0\uc6d0\ud558\uc9c0 \uc54a\uc73c\ubbc0\ub85c \uad6c\ud604 (\uc800\uc790\ucf54\ub4dc) class ExtTransformerEncoder d_model, d_ff, heads, dropout, num_inter_layers =0 TransformerEncoderLayer\ub97c \uc8fc\uc5b4\uc9c4 \ud69f\uc218\ub9cc\ud07c \ubc18\ubcf5\ud558\ub294 class","title":"1) modeling_bertext.py"},{"location":"presumm/#2-ext_trainpy","text":"\ubaa8\ub378\uacfc \ub370\uc774\ud130\uc14b\uc744 \ud1b5\ud574 train\uc744 \uc9c4\ud589\ud558\uba70, \uc2e4\uc9c8\uc801 \ud6c8\ub828\uc5d0 \ub300\ud55c \ucf54\ub4dc\ub294 ext_trainer.py\uc758 train\uc5d0 \uc815\uc758\ub418\uc5b4 \uc788\ub2e4. def main args: local_rank, train_path, val_path, max_len, language, ext_layers, ext_heades, ext_ff_size, ext_dropout, param_init_glorot, device Parameters args \u200b BertExt \ubaa8\ub378 \uad6c\uc131 \ud30c\ub77c\ubbf8\ud130 max_len ext_heads ext_ff_size ext_dropout ext_layers param_init_glorot language train_path / val_path: Train Data / Validation Data \uacbd\ub85c max_len: \uc694\uc57d \ub300\uc0c1 \ucd5c\ub300 \uae38\uc774 language: \uc5b8\uc5b4","title":"2) ext_train.py"},{"location":"presumm/#3-ext_trainerpy","text":"class SequentialDistributedSampler (Sampler): Sampler\ub294 Index\ub97c \ub2e4\ub8e8\ub294 \ubc29\ubc95\uc774\uba70, \ub370\uc774\ud130\uc758 Index\ub97c \uc6d0\ud558\ub294 \ubc29\uc2dd\ub300\ub85c \uc870\uc815\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. Sequential\ud55c \uc21c\uc11c\ub85c DataLoader\ub97c \uad6c\uc131\ud558\uba70, Distributed Setting\uc5d0 \uac78\ub9de\uac8c \uc9c4\ud589\ud569\ub2c8\ub2e4. def train args, model, train_dataset, eval_dataset trainer\uc758 train \ud568\uc218 \uacfc\uc815\uc740 train_dataset\uc744 \ubc30\uce58\ub2e8\uc704\ub85c \uc21c\ud68c\ud558\uba74\uc11c Model\uc758 Input\uc73c\ub85c \uc81c\uacf5\ud55c\ub2e4. \uc774\ud6c4 backward, step\uacfc \uac19\uc740 \uc77c\ubc18\uc801\uc778 Update\uacfc\uc815\uc744 \uc9c4\ud589\ud558\uace0 \ud6c8\ub828\uc774 \uc644\ub8cc\uac00 \ub418\uba74 train.py\uc5d0\uc11c global_step, tr_loss, best_val_loss, best_step = train(args, model, train_dataset, val_dataset,) \uc640 \uac19\uc740 \uaf34\ub85c \uc0ac\uc6a9\ud55c\ub2e4. Parameters args train_batch_size train_batch_size_per_gpu val_batch_size gpus language local_rank num_workers max_steps num_train_epochs gradient_accumulation_steps: gradient accumulation \ud69f\uc218 warmup_steps warmup_percent lr fp16 max_grad_norm description model: \ud6c8\ub828\ud558\uace0\uc790 \ud558\ub294 Model train_dataset eval_dataset def evaluate args, model, eval_datasets, prefix=\"\" train\uacfc \uac19\uc740 \uaf34\uc758 \ud568\uc218 \uad6c\uc131\uc744 \uac16\uc9c0\ub9cc update\uacfc\uc815 \uc5c6\uc774 loss\ub9cc return\ud55c\ub2e4. Parameters args checkpoint_dir local_rank val_batch_size val_batch_size_per_gpu gpus num_workers language device model eval_datasets prefix","title":"3) ext_trainer.py"},{"location":"presumm/#4-ext_testpy","text":"Test Data\ub97c Model Checkpoint\ub85c \uad6c\uc131\ud55c \ubaa8\ub378\uc758 Input\uc73c\ub85c \ub123\uc5b4 Inference\ub97c \uc9c4\ud589\ud558\uc5ec, ROUGE Score\ub97c \uacc4\uc0b0\ud574\ub0b8\ub2e4. def main args: test_data_path, save_path, device, ext_layers, ext_heads, ext_ff_size, ext_dropout, param_init_glorot, max_len, language, seed, local_rank, checkpoint_dir, val_batch_size BertExt \ubaa8\ub378 \uad6c\uc131 \ud30c\ub77c\ubbf8\ud130 max_len ext_heads ext_ff_size ext_dropout ext_layers param_init_glorot language path \uad00\ub828 \ud30c\ub77c\ubbf8\ud130 test_data_path save_path: format_rouge_scores return \uac12 \uc800\uc7a5 \uc704\uce58 checkpoint_dir: model ckpt \uc704\uce58 test_dataloader \uad00\ub828 \ud30c\ub9c8\ud2f0\ub7ec val_batch_size def format_rouge_scores scores Rouge-(1,2,L) \uc810\uc218\ub97c \ubaa8\ub450 Dictionary\ub85c \uac16\uace0 \uc788\ub294 scores\ub97c Input\uc73c\ub85c \ubc1b\uc544, \uc544\ub798\uc758 \uc608\uc2dc\uc640 \uac19\uc740 \uaf34\ub85c Return def save_rouge_scores str_scores format_rouge_scores\ub85c ROUGE SCORE\ub97c Formatting\ud55c \uacb0\uacfc\ub97c \ubc1b\uc544 txt file\ub85c \uc800\uc7a5","title":"4) ext_test.py"},{"location":"presumm/#5-abs-abstractive-summarization","text":"\uc0ac\uc6a9\ub418\ub294 \ucf54\ub4dc\ub294 \ud06c\uac8c 5\uac1c\ub85c \ub2e4\uc74c\uacfc \uac19\uc740 \uad6c\uc870\ub97c \uac00\uc9c0\uace0\uc788\uc73c\uba70 modeling_bertabs\ub294 abstractive summarization\uc744 \uc704\ud55c \ubaa8\ub378\uc774, abs_trainer\uc640 abs_train\uc740 \ud559\uc2b5 \uacfc\uc815\uc5d0 \ud544\uc694\ud55c \uc804\ubc18\uc801\uc778 \ucf54\ub4dc\ub97c, abs_generate\uc640 abs_inference\ub294 \ucd94\ub860 \uacfc\uc815\uc5d0\uc11c \ud544\uc694\ud55c \uc804\ubc18\uc801\uc778 \ucf54\ub4dc\ub97c \ub098\ud0c0\ub0b8\ub2e4.","title":"5. Abs - Abstractive Summarization"},{"location":"presumm/#1-modeling_bertabspy","text":"Abstractive \ub97c \uc704\ud55c \ubaa8\ub378\uc740 BertAbs class \ub0b4\ubd80\uc5d0 \uc815\uc758\ub418\uc5b4\uc788\uc73c\uba70 \ubaa8\ub378\uc758 \uad6c\uc870\ub294 Presumm \uc800\uc790\ucf54\ub4dc\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub418 \ud55c\uad6d\uc5b4 \ubaa8\ub378\uc744 \uad6c\ud604\ud558\ub294 \uacfc\uc815\uc5d0\uc11c \uba87\uac00\uc9c0 \ubd80\ubd84\uc744 \uc218\uc815\ud558\uc600\uc74c def get_kobert_vocab \uc544\ub798\uc640 \uac19\uc740 \ucf54\ub4dc\ub85c \ubaa8\ub378\uacfc vocab\uc744 \ud568\uaed8 \ubd88\ub7ec\uc624\ub294\uac83\uc774 \uc77c\ubc18\uc801\uc774\ub098, decoder part\uc5d0\uc11c BOS, EOS token\uc744 \ucd94\uac00\ud574\uc8fc\uc5b4\uc57c\ud558\uae30\ub54c\ubb38\uc5d0 vocab\uc744 \ub530\ub85c \ubd88\ub7ec\uc624\ub294 \ud568\uc218\ub97c \uc7ac \uc815\uc758\ud568 bert, vocab = get_pytorch_kobert_model() \ud568\uc218\uc5d0\uc11c \uc544\ub798\uc640 \uac19\uc740 \ucf54\ub4dc\ub97c \ud1b5\ud574 \uac04\ub2e8\ud788 \ub450\uac1c\uc758 token\uc744 \ucd94\uac00\ud560 \uc218 \uc788\uc73c\uba70, gluonnlp\uc758 https://nlp.gluon.ai/_modules/gluonnlp/vocab/bert.html \ub9c1\ud06c\ub97c \ucc38\uc870 \ud558\uc600\uc74c vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece( vocab_file, padding_token=\"[PAD]\", bos_token=\"[BOS]\", eos_token=\"[EOS]\" ) Class BertAbs input max_pos dec_layers dec_hidden_size dec_heads dec_ff_size dec_dropout language : Korean checkpoint_dir : pretrained Encoder\ub97c \ubd88\ub7ec\uc62c \ub54c \uc0ac\uc6a9 def forward encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask Dataset\uc73c\ub85c\ubd80\ud130 \ub098\uc624\ub294 src_doc_padded = encoder_input_ids, tgt_doc_padded = decoder_input_ids, src_doc_type_padded = token_type_ids, src_doc_mask = encoder_attention_mask\uc5d0 \uac01\uac01 \ub9e4\uce6d\ub41c\ub2e4","title":"1) modeling_bertabs.py"},{"location":"presumm/#1-encoder","text":"bert \ubaa8\ub378\uc744 \ud1b5\uacfc\ud558\uc5ec \uc5bb\uc5b4\uc9c4 encoder_output \uc73c\ub85c \ubd80\ud130 encoder_hidden_states \ub97c \uc5bb\uc744\uc218 \uc788\ub2e4","title":"1) Encoder"},{"location":"presumm/#2-decoder","text":"","title":"2) Decoder"},{"location":"presumm/#2-1-decoder-initial-state","text":"encoder_input_ids`` , encoder_hidden_states`\ub97c \ud65c\uc6a9\ud558\uc5ec decoder\uc758 initial state\ub97c \uc5bb\uace0","title":"2-1) Decoder initial state"},{"location":"presumm/#2-2-decoder-output","text":"decoder_input_ids , encoder_hidden_states , dec_state \ub97c \uc0ac\uc6a9\ud558\uc5ec decoder_outputs \ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4","title":"2-2) Decoder output"},{"location":"presumm/#2-3-generator","text":"decoder_output \uc744 \ud65c\uc6a9\ud558\uc5ec Linear layer\ub97c \uac70\uccd0 hidden vector\ub97c vocab\uc73c\ub85c \ubcc0\ud658\ud558\ub294 \uacfc\uc815\uc744 \uac70\uce5c\ub2e4 Class TransformerDecoder \ubcf8 \ucf54\ub4dc\ub294 \"Attention is All You Need\" \ub17c\ubb38\uc758 \ucf54\ub4dc\ub97c \uadf8\ub300\ub85c \uac00\uc838\uc640 \uc0ac\uc6a9\ud558\uc600\uc73c\uba70 \ubcf8 \ucf54\ub4dc\ub294 scratch\ubd80\ud130 transformer decoder\ub97c \uad6c\ud604\ud55c \ucf54\ub4dc\uc774\ub2e4. https://arxiv.org/abs/1706.03762\ub97c \ud1b5\ud574 \uad6c\uc870\ub97c \uc774\ud574\ud560 \uc218 \uc788\ub2e4.","title":"2-3) Generator"}]}