
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-6.1.4">
    
    
      
        <title>Presumm - My Docs</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.358818c7.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.f0267088.min.css">
        
      
    
    
    
      
        
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
      
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#presumm" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href=".." title="My Docs" class="md-header-nav__button md-logo" aria-label="My Docs">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            My Docs
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Presumm
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="My Docs" class="md-nav__button md-logo" aria-label="My Docs">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    My Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." class="md-nav__link">
      한국어 문서요약
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../BERTSUM/" class="md-nav__link">
      BertSum
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../Neusum/" class="md-nav__link">
      Neusum
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../PointerGenerator/" class="md-nav__link">
      PointerGenerator
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../TextRank/" class="md-nav__link">
      TextRank
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../docs-pointergenerator/" class="md-nav__link">
      Docs pointergenerator
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Presumm
        <span class="md-nav__icon md-icon"></span>
      </label>
    
    <a href="./" class="md-nav__link md-nav__link--active">
      Presumm
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-overview" class="md-nav__link">
    1. Overview
  </a>
  
    <nav class="md-nav" aria-label="1. Overview">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-presumm-for-extractive-summarization" class="md-nav__link">
    1) Presumm for Extractive summarization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-presumm-for-abstractive-summarization" class="md-nav__link">
    2) Presumm for Abstractive summarization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-code-scheme" class="md-nav__link">
    2. Code scheme
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-utils" class="md-nav__link">
    3. Utils
  </a>
  
    <nav class="md-nav" aria-label="3. Utils">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-datasetpy" class="md-nav__link">
    1) dataset.py
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-greedypy" class="md-nav__link">
    2) greedy.py
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-modeling_utilspy" class="md-nav__link">
    3) modeling_utils.py
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-preprocesspy" class="md-nav__link">
    4) preprocess.py
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-ext-extractive-summarization" class="md-nav__link">
    4. Ext - Extractive Summarization
  </a>
  
    <nav class="md-nav" aria-label="4. Ext - Extractive Summarization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-modeling_bertextpy" class="md-nav__link">
    1) modeling_bertext.py
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-ext_trainpy" class="md-nav__link">
    2) ext_train.py
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-ext_trainerpy" class="md-nav__link">
    3) ext_trainer.py
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-ext_testpy" class="md-nav__link">
    4) ext_test.py
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-abs-abstractive-summarization" class="md-nav__link">
    5. Abs - Abstractive Summarization
  </a>
  
    <nav class="md-nav" aria-label="5. Abs - Abstractive Summarization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-modeling_bertabspy" class="md-nav__link">
    1) modeling_bertabs.py
  </a>
  
    <nav class="md-nav" aria-label="1) modeling_bertabs.py">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-encoder" class="md-nav__link">
    1) Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-decoder" class="md-nav__link">
    2) Decoder
  </a>
  
    <nav class="md-nav" aria-label="2) Decoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2-1-decoder-initial-state" class="md-nav__link">
    2-1) Decoder initial state
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-2-decoder-output" class="md-nav__link">
    2-2) Decoder output
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-3-generator" class="md-nav__link">
    2-3) Generator
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-overview" class="md-nav__link">
    1. Overview
  </a>
  
    <nav class="md-nav" aria-label="1. Overview">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-presumm-for-extractive-summarization" class="md-nav__link">
    1) Presumm for Extractive summarization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-presumm-for-abstractive-summarization" class="md-nav__link">
    2) Presumm for Abstractive summarization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-code-scheme" class="md-nav__link">
    2. Code scheme
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-utils" class="md-nav__link">
    3. Utils
  </a>
  
    <nav class="md-nav" aria-label="3. Utils">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-datasetpy" class="md-nav__link">
    1) dataset.py
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-greedypy" class="md-nav__link">
    2) greedy.py
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-modeling_utilspy" class="md-nav__link">
    3) modeling_utils.py
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-preprocesspy" class="md-nav__link">
    4) preprocess.py
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-ext-extractive-summarization" class="md-nav__link">
    4. Ext - Extractive Summarization
  </a>
  
    <nav class="md-nav" aria-label="4. Ext - Extractive Summarization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-modeling_bertextpy" class="md-nav__link">
    1) modeling_bertext.py
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-ext_trainpy" class="md-nav__link">
    2) ext_train.py
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-ext_trainerpy" class="md-nav__link">
    3) ext_trainer.py
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-ext_testpy" class="md-nav__link">
    4) ext_test.py
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-abs-abstractive-summarization" class="md-nav__link">
    5. Abs - Abstractive Summarization
  </a>
  
    <nav class="md-nav" aria-label="5. Abs - Abstractive Summarization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-modeling_bertabspy" class="md-nav__link">
    1) modeling_bertabs.py
  </a>
  
    <nav class="md-nav" aria-label="1) modeling_bertabs.py">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-encoder" class="md-nav__link">
    1) Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-decoder" class="md-nav__link">
    2) Decoder
  </a>
  
    <nav class="md-nav" aria-label="2) Decoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2-1-decoder-initial-state" class="md-nav__link">
    2-1) Decoder initial state
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-2-decoder-output" class="md-nav__link">
    2-2) Decoder output
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-3-generator" class="md-nav__link">
    2-3) Generator
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="presumm">Presumm</h1>
<h2 id="1-overview">1. Overview</h2>
<p>Presumm은 Bertsum의 저자가 발표한 후속 논문으로, Extractive summarization을 목적으로 한 Bertsum을 활용하여 <strong>Extractive summarization model</strong>과 <strong>Abstractive summarization</strong>이 모두 가능한 두가지 모델을 제안한다.</p>
<h4 id="1-presumm-for-extractive-summarization">1) Presumm for Extractive summarization</h4>
<ul>
<li>
<p>기존의 BERT input과 다르게 모든 문장 앞뒤에 <strong>[CLS], [SEP]</strong> token을 배치함</p>
</li>
<li>
<p>한국어 모델을 구현하기위해 SKT에서 공개한 KoBERT 모델을 활용함</p>
</li>
<li>KoBERT의 아웃풋 벡터를 모두 사용하지 않고 CLS token의 Top vector만 사용하여 Transformer Encoder Layer를 통과함 (사용한 레이어 갯수는 2개)</li>
</ul>
<p><img alt="70D5C5A1-62E3-46BF-9DE8-F461FDCD70E0_1_105_c" src="../presumm.assets/70D5C5A1-62E3-46BF-9DE8-F461FDCD70E0_1_105_c.jpeg" /></p>
<h4 id="2-presumm-for-abstractive-summarization">2) Presumm for Abstractive summarization</h4>
<ul>
<li>Encoder는 KoBERT 혹은 Pretrained Extractive Summarization Model을 사용하고, Decoder는 Transformer Decoder를 사용하는 모델 구조</li>
<li>Encoder의 Input은 Extractive model과 동일하게 하나의 문장 앞뒤에 <strong>[CLS], [SEP]</strong> token을 붙임</li>
<li>Decoder는 별도의 Special token을 사용하지 않지만, 문장 앞뒤에 <strong>[BOS], [EOS]</strong>를 더하여 Document의 시작과 끝을 표시함</li>
<li>Extractive model과 다르게 Document 전체 벡터를 모두 활용하여 Decoder에 넣어줌 </li>
<li>Beam search를 활용하여 최종 Output vocab을 찾아냄</li>
</ul>
<p><img alt="5D4D60FB-BE71-4455-B650-A9D1F7342650" src="../presumm.assets/5D4D60FB-BE71-4455-B650-A9D1F7342650.jpeg" /></p>
<h2 id="2-code-scheme">2. Code scheme</h2>
<pre><code class="language-python">Presumm
├── requirements.txt
├── utils
│   ├── dataset.py
│   ├── greedy.py
│   ├── modeling_utils.py
│   └── preprocess.py
├── ext
│   ├── modeling_bertext.py
│   ├── ext_train.py
│   ├── ext_trainer.py
│   ├── inference.py
│   └── ext_test.py
├── abs
│   ├── modeling_bertabs.py
│   ├── abs_train.py
│   ├── abs_trainer.py
│   ├── abs_test.py
│   ├── abs_generate.py
│   └── train.sh
</code></pre>
<h2 id="3-utils">3. Utils</h2>
<h4 id="1-datasetpy">1) dataset.py</h4>
<blockquote>
<p><em>Class</em> <strong>Bertsum_Dataset</strong>  </p>
<p><code>data_path : str, max_len = 512 , train = True, mode = 'ext', language = 'kor'</code></p>
</blockquote>
<p>Bertsum에 사용될 데이터셋 class이며, Extractive summarization과 Abstract summarization 모두 적용 가능한 함수로 <code>mode</code> 를 정확히 바꾸어주어야한다. 또한 학습/추론 여부에 따라 <code>Train</code> option을 바꾸어서 사용해야하며 Dataloader를 사용할때 Dataset 옵션에 해당 class를 import하고 사용해야한다.</p>
<ul>
<li>Parameters</li>
<li>data_path ( <code>str</code> ,  require)</li>
<li>max_len ( <code>int</code> ,optional, defaults to 512 )</li>
<li>train (<code>bool</code> , optional, defaults to True )</li>
<li>mode ( <code>str</code> ) if string,  <code>ext</code>, <code>abs</code> are supported</li>
<li>language ( <code>str</code> , defaults to 'kor') </li>
</ul>
<blockquote>
<p><em>def</em>  <strong>Collate</strong> </p>
<p><code>batch</code></p>
</blockquote>
<p>Collate 함수는 보통 Data Loader의 배치 내 데이터들이 길이나 구성을 같게 해주는 함수이다. 해당 Collate Function이 진행하는 처리는 데이터간 길이를 맞춰주기 위해 패딩을 실시해야 하는데 문장이 Max Length를 넘어갈 경우, 해당 Max Length 이하의 마지막 [CLS] Token의 위치를 찾아 문장 패딩을 진행한다.</p>
<blockquote>
<p><em>def</em> <strong>src_add_pad</strong></p>
<p><code>token_ids, token_type</code></p>
</blockquote>
<p><code>token_ids</code>는 전체 document를 token화 한 후 vocab에 따라 id로 변경한 것을 의미하며, <code>token_type</code>은 document의 문장을 구별해주는 식별자이다. 해당 함수는 source document를 max_length에 따라 padding을 진행해주는 함수</p>
<blockquote>
<p><em>def</em> <strong>tgt_add_pad</strong></p>
<p><code>token_ids</code> </p>
</blockquote>
<p>해당 함수는 target document를 max_length에 따라 padding을 진행해주는 함수</p>
<blockquote>
<p><em>def</em>  <strong>add_special_token</strong></p>
<p><code>token_ids</code> </p>
</blockquote>
<p>해당 함수는 source document에서 한 문장마다  special token인 [CLS]와 [SEP]로 감싸주는 함수</p>
<blockquote>
<p><em>def</em>  <strong>add_sentence_token</strong></p>
<p><code>token_ids</code> </p>
</blockquote>
<p>해당 함수는 target document 전체를 [BOS], [EOS]로 감싸주는 역할을 하며 문장단위가 아니라 전체 document를 감싸주는 함수</p>
<blockquote>
<p><em>def</em>  <strong>idx2mask</strong></p>
<p><code>token_ids</code> </p>
</blockquote>
<p>padding index로 이루어진 sequence를 mask로 바꾸어주는 함수</p>
<blockquote>
<p><em>def</em>  <strong>get_token_type_ids</strong></p>
<p><code>src_token_padded</code></p>
</blockquote>
<p>source document의 seuquence가 존재할 때 문장 단위로 구별하기 위해 type id를 부여하는 함수</p>
<blockquote>
<p><em>def</em>  <strong>get_cls_index</strong></p>
<p><code>src_token_padded</code></p>
</blockquote>
<p>Extractive summarization에서만 사용되는 함수로 cls token의 위치를 알아내기위해 cls index 위치를 찾는 함수</p>
<blockquote>
<p><em>def</em>  <strong><em> _ getitem _ </em></strong></p>
<p><code>idx</code></p>
</blockquote>
<p>Pytorch Dataset을 만들기위한 함수로, 학습과 추론시 batch 단위로 데이터를 불러올 수 있도록 구성되어있음</p>
<p>1) Extractive Summarization Output (Train/Test동일)</p>
<pre><code class="language-python">batch = [
         torch.tensor(src_doc_padded),
         torch.tensor(src_doc_mask),
         torch.tensor(src_doc_type_padded),
         torch.tensor(cls_index),
         torch.tensor(ext_labels),
         src_summary,
         tgt_doc_summary
        ]
</code></pre>
<p>아래 세 종류(src / cls, ext / summary)의 데이터는 BERT의 input으로 활용</p>
<ul>
<li>src_doc_padded : padding을 완료한 source document의 sequence</li>
<li>src_doc_mask : source document의 mask sequence </li>
<li>
<p>src_doc_type_padded : source document 문장의 token type sequence </p>
</li>
<li>
<p>cls_index : source document에서 cls token의 index</p>
</li>
<li>
<p>ext_labels : Extractive summarization label</p>
</li>
<li>
<p>src_summary, tgt_doc_summary : 원본 텍스트</p>
</li>
</ul>
<p>2-1) Abstractive Summarization Output (Train)</p>
<pre><code class="language-python">batch = [
         torch.tensor(src_doc_padded),
         torch.tensor(src_doc_mask),
         torch.tensor(src_doc_type_padded),
         torch.tensor(tgt_doc_padded),
         torch.tensor(tgt_doc_mask),
            ]
</code></pre>
<p>아래 세가지 데이터는 BERT (encoder) 의 input으로 활용</p>
<ul>
<li>src_doc_padded : padding을 완료한 source document의 sequence</li>
<li>src_doc_mask : source document의 mask sequence </li>
<li>src_doc_type_padded : source document 문장의 token type sequence </li>
</ul>
<p>아래 두가지 데이터는  decoder 의 input으로 활용</p>
<ul>
<li>tgt_doc_padded : padding을 완료한 target document의 sequence</li>
<li>tgt_doc_mask : target document의 mask sequence </li>
</ul>
<p>2-2) Abstractive summarization output (Test)</p>
<pre><code class="language-python">batch = [
         torch.tensor(src_doc_padded),
         torch.tensor(src_doc_mask),
         torch.tensor(src_doc_type_padded),
         torch.tensor(tgt_doc_padded),
         torch.tensor(tgt_doc_mask),
         tgt_doc_summary
        ]
</code></pre>
<p>아래 세가지 데이터는 BERT (encoder) 의 input으로 활용</p>
<ul>
<li>src_doc_padded : padding을 완료한 source document의 sequence</li>
<li>src_doc_mask : source document의 mask sequence </li>
<li>src_doc_type_padded : source document 문장의 token type sequence </li>
</ul>
<p>아래 두가지 데이터는  test 과정에서 실제로 사용되지않음</p>
<ul>
<li>tgt_doc_padded : padding을 완료한 target document의 sequence</li>
<li>
<p>tgt_doc_mask : target document의 mask sequence </p>
</li>
<li>
<p>tgt_doc_summary : 원본 텍스트</p>
</li>
</ul>
<h4 id="2-greedypy">2) greedy.py</h4>
<p>Extractive Summarization을 진행하기 위해선 Extractive Summary Sentences가 필요하며, 이는 보편적으로 원문과 Gold summary(abstractive)의 <strong>Greedy Selection</strong>을 통해 이루어진다. </p>
<p><strong>Greedy Selection</strong>은 원문의 문장들을 순회하며 Gold summary와의 ROUGE-2 score를 계산한 뒤 순위를 매긴다. 그 후 순위가 높은 순으로 기준 개수 만큼 선택하며, BertSum에서는 top-3 문장을 선택했다.</p>
<blockquote>
<p>def _get_ngrams(n, text)</p>
</blockquote>
<ul>
<li>Parameters</li>
<li>n: 몇개의 token을 추출한 것인지에 대한 파라미터 (<strong>n</strong>-gram)</li>
<li>text: ngram 추출 대상 문장</li>
</ul>
<blockquote>
<p>def _get_word_ngrams(n, sentences)</p>
</blockquote>
<ul>
<li>Parameters</li>
<li>n: 몇개의 token을 추출한 것인지에 대한 파라미터 (<strong>n</strong>-gram)</li>
<li>sentences: 몇 개 문장을 대상으로 ngram을 구할 지에 대한 파라미터</li>
</ul>
<blockquote>
<p>def cal_rouge(evaluated_ngrams, reference_ngrams)</p>
</blockquote>
<ul>
<li>Parameters</li>
<li>evaluated_ngrams: 원문 문장 tokens</li>
<li>reference_ngrams: 생성 요약 문장 tokens</li>
</ul>
<blockquote>
<p>def greedy_selection(doc_sent_list, abstract_sent_list, summary_size)</p>
</blockquote>
<ul>
<li>
<p>Parameters</p>
</li>
<li>
<p>doc_sent_list: 원문 문장별 분할 리스트</p>
</li>
<li>
<p>abstract_sent_list: 생성요약 문장별 분할 리스트</p>
</li>
<li>
<p>summary_size: 추출 요약 문장 갯수</p>
</li>
</ul>
<blockquote>
<p>def greedy_extract(doc_sent_list, abstract_sent_list, summary_size):</p>
</blockquote>
<ul>
<li>Parameters</li>
<li>
<p>doc_sent_list: 원문 문장별 분할 리스트</p>
</li>
<li>
<p>abstract_sent_list: 생성요약 문장별 분할 리스트</p>
</li>
<li>
<p>summary_size: 추출 요약 문장 갯수</p>
</li>
</ul>
<h4 id="3-modeling_utilspy">3) modeling_utils.py</h4>
<blockquote>
<p><em>def</em>  <strong>set_seed</strong></p>
<p><code>args</code></p>
</blockquote>
<p>modeling에 사용되는 모든 랜덤시드를 고정하는 함수</p>
<blockquote>
<p><em>def</em>  <strong>save_pkl</strong></p>
<p><code>path</code> , <code>file</code></p>
</blockquote>
<p>pickle 파일을 저장하는 함수 </p>
<blockquote>
<p><em>def</em>  <strong>load_pkl</strong></p>
<p><code>path</code> , <code>file</code></p>
</blockquote>
<p>pickle 파일을 로드하는 함수 </p>
<h4 id="4-preprocesspy">4) preprocess.py</h4>
<blockquote>
<p><em>Class</em> <strong>Korean_Dataset</strong></p>
<p>Train, Valid, Test에 맞게 데이터셋을 전처리 하는 클래스. </p>
<blockquote>
<p><em>def</em> <strong>parse_data</strong></p>
<p><code>data_type</code> , <code>save_path</code></p>
</blockquote>
<p>원본 데이터를 불러온 다음 korean tokenizer를 활용하여 데이터를 token화 하는 함수. output으로는 pickle 형태의 파일이 저장</p>
<p>저장된 파일은 dictionary 형태로 5가지 key를 가진다</p>
<ul>
<li>src_tokens : tokenize된 source document</li>
<li>tgt_tokens : tokenize된 target docuement</li>
<li>
<p>src_raw : 원본 source document</p>
</li>
<li>
<p>tgt_raw : 원본 target docuement</p>
</li>
<li>ext_labels : greedy selection을 진행한 extractive summarization label</li>
</ul>
</blockquote>
<h2 id="4-ext-extractive-summarization">4. Ext - Extractive Summarization</h2>
<p>사용되는 코드는 크게 4개로 다음과 같은 구조와 목적을 갖고 있다. </p>
<ul>
<li>modeling_bertext: Extractive Summarization Model</li>
<li>ext_trainer / ext_Train: 학습 과정에 필요한 전반적인 코드</li>
<li>ext_test: 추론 과정과 ROUGE Score 계산에 필요한 코드</li>
</ul>
<h4 id="1-modeling_bertextpy">1) modeling_bertext.py</h4>
<blockquote>
<p><em>class</em> <strong>BertExt</strong></p>
<p><code>max_pos : int, ext_layers : int, ext_heads : int, ext_ff_size : int, ext_dropout : int, param_init_glorot = True, language = 'kor'</code></p>
</blockquote>
<p>BertSum Extractive Model을 구성하는 Class이다. <code>language</code>에 따라 구성 모델의 언어가 바뀌어 사용하는 Bert가 달라지며, 나머지 파라미터들은 BertSum Extractive Model에 포함되는 Transformer의 구조를 다양하게 조절하는 파라미터들이다. </p>
<ul>
<li>
<p>Parameters</p>
</li>
<li>
<p>max_pos: 문장별 최대 길이</p>
</li>
<li>ext_layers: Extractive Transformer Encoder Layer 개수</li>
<li>ext_heads: MultiHeadAttention Parallel Heads 개수</li>
<li>ext_ff_size: PositionwiseFeadForward Hidden Layers 갯수</li>
<li>ext_dropout: PositionwiseFeadForward Dropout Rate</li>
<li>param_init_glorot: Layer별 Xavier_uniform 적용 여부</li>
<li>language(<code>str</code>, optional, default to <code>kor</code>) - BERT 또는 KoBERT 사용 유무</li>
</ul>
<blockquote>
<p>def forward</p>
<p><code>encoder_input_ids, encoder_attention_mask, token_type_ids, cls_index, cls_mask, ext_labels</code></p>
</blockquote>
<p>BERT(또는 KoBERT)에 token id(encoder_input_ids) / attention mask(encoder_attention_mask) / 문장 구분 id (token_type_ids)를 입력으로 받아, 문장의 BERT Encoding 값을 top_vec으로 return 받고, CLS Token의 값만을 필터링하여 Transformer Layer를 통과해 문장별 점수를 Return합니다.</p>
<blockquote>
<p>class PositionwiseFeedForward</p>
<p><code>head_count, model_dim, dropout=0.1, use_final_linear=True</code></p>
</blockquote>
<p>Transformer의 PositionwiseFeedForward Function (저자코드)</p>
<blockquote>
<p>class MultiHeadedAttention</p>
</blockquote>
<p>Transformer의 MultiHeadedAttention</p>
<blockquote>
<p>class Classifier</p>
<p><code>hidden_size</code></p>
</blockquote>
<p>BERT의 Last Layer를 통해 분류 문제 해결 Function</p>
<blockquote>
<p>class PositionalEncoding</p>
<p><code>dropout, dim, max_len-5000</code></p>
</blockquote>
<p>Transformer의 PositionalEncoding</p>
<blockquote>
<p>class TransformerEncoderLayer</p>
<p><code>d_model, heads, d_ff, dropout</code></p>
</blockquote>
<p>Transformer의 EncodingLayer를 구현한 것인데, 이는 HuggingFace에서 지원하지 않으므로 구현 (저자코드)</p>
<blockquote>
<p>class ExtTransformerEncoder</p>
<p><code>d_model, d_ff, heads, dropout, num_inter_layers =0</code></p>
</blockquote>
<p>TransformerEncoderLayer를 주어진 횟수만큼 반복하는 class</p>
<h4 id="2-ext_trainpy">2) ext_train.py</h4>
<p>모델과 데이터셋을 통해 train을 진행하며, 실질적 훈련에 대한 코드는 ext_trainer.py의 train에 정의되어 있다.</p>
<blockquote>
<p><em>def</em> <strong>main</strong></p>
<p><code>args: local_rank, train_path, val_path,  max_len, language, ext_layers, ext_heades, ext_ff_size, ext_dropout, param_init_glorot, device</code></p>
</blockquote>
<ul>
<li>
<p>Parameters</p>
</li>
<li>
<p>args</p>
<p>​   BertExt 모델 구성 파라미터</p>
<ul>
<li>max_len</li>
<li>ext_heads</li>
<li>ext_ff_size</li>
<li>ext_dropout</li>
<li>ext_layers</li>
<li>param_init_glorot</li>
<li>language</li>
<li>train_path / val_path: Train Data / Validation Data 경로</li>
<li>max_len: 요약 대상 최대 길이</li>
<li>language: 언어</li>
</ul>
</li>
</ul>
<h4 id="3-ext_trainerpy">3) ext_trainer.py</h4>
<blockquote>
<p><em>class</em> <strong>SequentialDistributedSampler</strong>(Sampler):</p>
</blockquote>
<p>Sampler는 Index를 다루는 방법이며, 데이터의 Index를 원하는 방식대로 조정하는 것입니다. Sequential한 순서로 DataLoader를 구성하며, Distributed Setting에 걸맞게 진행합니다.</p>
<blockquote>
<p><em>def</em> <strong>train</strong></p>
<p><code>args, model, train_dataset, eval_dataset</code></p>
</blockquote>
<p>trainer의 train 함수 과정은 train_dataset을 배치단위로 순회하면서 Model의 Input으로 제공한다. 이후 backward, step과 같은 일반적인 Update과정을 진행하고 훈련이 완료가 되면 train.py에서 </p>
<p><code>global_step, tr_loss, best_val_loss, best_step = train(args, model, train_dataset, val_dataset,)</code></p>
<p>와 같은 꼴로 사용한다.</p>
<ul>
<li>
<p>Parameters</p>
</li>
<li>
<p>args</p>
<ul>
<li>
<p>train_batch_size</p>
</li>
<li>
<p>train_batch_size_per_gpu</p>
</li>
<li>
<p>val_batch_size</p>
</li>
<li>
<p>gpus</p>
</li>
<li>
<p>language</p>
</li>
<li>
<p>local_rank</p>
</li>
<li>
<p>num_workers</p>
</li>
<li>
<p>max_steps</p>
</li>
<li>
<p>num_train_epochs</p>
</li>
<li>
<p>gradient_accumulation_steps: gradient accumulation 횟수</p>
</li>
<li>
<p>warmup_steps</p>
</li>
<li>
<p>warmup_percent</p>
</li>
<li>
<p>lr</p>
</li>
<li>
<p>fp16</p>
</li>
<li>
<p>max_grad_norm</p>
</li>
<li>
<p>description</p>
</li>
</ul>
</li>
<li>
<p>model: 훈련하고자 하는 Model</p>
</li>
<li>
<p>train_dataset</p>
</li>
<li>
<p>eval_dataset</p>
</li>
</ul>
<blockquote>
<p><em>def</em> <strong>evaluate</strong></p>
<p><code>args, model, eval_datasets, prefix=""</code></p>
</blockquote>
<p>train과 같은 꼴의 함수 구성을 갖지만 update과정 없이 loss만 return한다.</p>
<ul>
<li>Parameters</li>
<li>args<ul>
<li>checkpoint_dir</li>
<li>local_rank</li>
<li>val_batch_size</li>
<li>val_batch_size_per_gpu</li>
<li>gpus</li>
<li>num_workers</li>
<li>language</li>
<li>device</li>
</ul>
</li>
<li>model</li>
<li>eval_datasets</li>
<li>prefix</li>
</ul>
<h4 id="4-ext_testpy">4) ext_test.py</h4>
<p>Test Data를 Model Checkpoint로 구성한 모델의 Input으로 넣어 Inference를 진행하여, ROUGE Score를 계산해낸다.</p>
<blockquote>
<p>def <strong>main</strong></p>
<p><code>args: test_data_path, save_path, device, ext_layers, ext_heads, ext_ff_size, ext_dropout, param_init_glorot, max_len, language, seed, local_rank, checkpoint_dir, val_batch_size</code></p>
</blockquote>
<ul>
<li>BertExt 모델 구성 파라미터</li>
<li>max_len</li>
<li>ext_heads</li>
<li>ext_ff_size</li>
<li>ext_dropout</li>
<li>ext_layers</li>
<li>param_init_glorot</li>
<li>language</li>
<li>path 관련 파라미터</li>
<li>test_data_path</li>
<li>save_path: format_rouge_scores return 값 저장 위치</li>
<li>
<p>checkpoint_dir: model ckpt 위치</p>
</li>
<li>
<p>test_dataloader 관련 파마티러</p>
</li>
<li>val_batch_size</li>
</ul>
<blockquote>
<p><em>def</em> <strong>format_rouge_scores</strong></p>
<p><code>scores</code></p>
</blockquote>
<p>Rouge-(1,2,L) 점수를 모두 Dictionary로 갖고 있는 scores를 Input으로 받아, 아래의 예시와 같은 꼴로 Return</p>
<p><img src="presumm.assets/image-20201215115753515.png" alt="image-20201215115753515" style="zoom:50%;" /></p>
<blockquote>
<p><em>def</em> <strong>save_rouge_scores</strong></p>
<p><code>str_scores</code></p>
</blockquote>
<p>format_rouge_scores로 ROUGE SCORE를 Formatting한 결과를 받아 txt file로 저장</p>
<h2 id="5-abs-abstractive-summarization">5. Abs - Abstractive Summarization</h2>
<p>사용되는 코드는 크게 5개로 다음과 같은 구조를 가지고있으며 modeling_bertabs는 abstractive summarization을 위한 모델이, abs_trainer와 abs_train은 학습 과정에 필요한 전반적인 코드를, abs_generate와 abs_inference는 추론 과정에서 필요한 전반적인 코드를 나타낸다.</p>
<p><img alt="C6C5B61C-4001-4EA0-9260-582F1A822EFF_1_105_c" src="../presumm.assets/C6C5B61C-4001-4EA0-9260-582F1A822EFF_1_105_c.jpeg" /></p>
<h4 id="1-modeling_bertabspy">1) modeling_bertabs.py</h4>
<p>Abstractive 를 위한 모델은 <strong>BertAbs</strong>  class 내부에 정의되어있으며 모델의 구조는 Presumm 저자코드를 기반으로 하되 한국어 모델을 구현하는 과정에서 몇가지 부분을 수정하였음 </p>
<blockquote>
<p><em>def</em> <strong>get_kobert_vocab</strong></p>
</blockquote>
<p>아래와 같은 코드로 모델과 vocab을 함께 불러오는것이 일반적이나, decoder part에서 BOS, EOS token을 추가해주어야하기때문에 vocab을 따로 불러오는 함수를 재 정의함</p>
<pre><code class="language-python">bert, vocab  = get_pytorch_kobert_model()
</code></pre>
<p>함수에서 아래와 같은 코드를 통해 간단히 두개의 token을 추가할 수 있으며, gluonnlp의 https://nlp.gluon.ai/_modules/gluonnlp/vocab/bert.html 링크를 참조 하였음</p>
<pre><code class="language-python">vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(
        vocab_file, padding_token=&quot;[PAD]&quot;, bos_token=&quot;[BOS]&quot;, eos_token=&quot;[EOS]&quot;
    )
</code></pre>
<blockquote>
<p><em>Class</em> <strong>BertAbs</strong></p>
</blockquote>
<ul>
<li>input </li>
<li>max_pos</li>
<li>dec_layers</li>
<li>dec_hidden_size</li>
<li>dec_heads</li>
<li>dec_ff_size</li>
<li>dec_dropout</li>
<li>language : Korean</li>
<li>checkpoint_dir : pretrained Encoder를 불러올 때 사용</li>
</ul>
<blockquote>
<p><em>def</em> <strong>forward</strong></p>
<p><code>encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask</code></p>
<p>Dataset으로부터 나오는 src_doc_padded = encoder_input_ids, tgt_doc_padded = decoder_input_ids, src_doc_type_padded = token_type_ids, src_doc_mask = encoder_attention_mask에 각각 매칭된다</p>
</blockquote>
<h5 id="1-encoder">1) Encoder</h5>
<p>bert 모델을 통과하여 얻어진 <code>encoder_output</code>으로 부터 <code>encoder_hidden_states</code>를 얻을수 있다</p>
<h5 id="2-decoder">2) Decoder</h5>
<h6 id="2-1-decoder-initial-state">2-1) Decoder initial state</h6>
<p><code>encoder_input_ids`` ,</code>encoder_hidden_states`를 활용하여 decoder의 initial state를 얻고</p>
<h6 id="2-2-decoder-output">2-2) Decoder output</h6>
<p><code>decoder_input_ids</code> , <code>encoder_hidden_states</code>, <code>dec_state</code>를 사용하여 <code>decoder_outputs</code>를 얻을 수 있다</p>
<h6 id="2-3-generator">2-3) Generator</h6>
<p><code>decoder_output</code>을 활용하여 Linear layer를 거쳐 hidden vector를 vocab으로 변환하는 과정을 거친다</p>
<blockquote>
<p><em>Class</em> <strong>TransformerDecoder</strong></p>
</blockquote>
<p>본 코드는 "Attention is All You Need" 논문의 코드를 그대로 가져와 사용하였으며 본 코드는 scratch부터 transformer decoder를 구현한 코드이다. https://arxiv.org/abs/1706.03762를 통해 구조를 이해할 수 있다.</p>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../docs-pointergenerator/" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Docs pointergenerator
              </div>
            </div>
          </a>
        
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.7e0ee788.min.js"></script>
      <script src="../assets/javascripts/bundle.b3a72adc.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: [],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>